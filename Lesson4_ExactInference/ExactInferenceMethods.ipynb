{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exact Inference Algorithms\n",
    "\n",
    "\n",
    "\n",
    "## CSCI E-83\n",
    "## Stephen Elston\n",
    "\n",
    "We have seen several approaches to representation of probabilistic graphical models. Now, we will turn our attention to **inference algorithms**. The goal of inference is to compute the **posterior distribution** of one or more variables in the model given **evidence**. Alternatively, we can say that inference is used to return results to a **query** on the model. \n",
    "\n",
    "In this discussion we differentiate between inference and learning. With Bayesian models, this distinction can be rather arbitrary, but fits the nature of the discussion herein. The role of inference in an intelligent agent is illustrated in the figure below. \n",
    "\n",
    "<img src=\"img/Inference.JPG\" alt=\"Drawing\" style=\"width:400px; height:200px\"/>\n",
    "<center> **Inference in an intelligent agent** </center>\n",
    "\n",
    "In this lesson we will examine three efficient classes of algorithms for inference on graphical models:\n",
    "\n",
    "1. **Variable elimination:**\n",
    "2. **Message passing, or sum-product or belief propagation algorithms:**\n",
    "3. **Junction tree algorithm:**\n",
    "\n",
    "\n",
    "**Suggested readings:** The following reading is an optional supplement to the material presented here:\n",
    "- Barber, Sections 5.1, 5.3, or\n",
    "- Murphy, Section 20.1, 20.2, 20.3, 20.4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complexity of inference for graphical models\n",
    "\n",
    "To understand the need for efficient inference algorithms it helps to understand the computational complexity of inference of a graphical model. If we use a **tabular** solution algorithm, the complexity is NP; meaning the number of operations required grow as order $= O(n^k)$. \n",
    "\n",
    "On the face of things, it might seem that performing inference on graphical models of any scale is hopeless. While it is true, there are no general algorithms for solving the inference problem, there are many practical and widely applicable cases for which efficient inference algorithms exist. \n",
    "\n",
    "The key to reducing the computational complexity of graphical model inference algorithms is use of independencies. The naive approach is to simple the full table of marginal distributions of the graph variables. This approach has combinatorial or NP complexity. By combining conditional probabilities with evidence, the complexity of marginal influence can be significantly reduced. \n",
    "\n",
    "The algorithms we will explore take advantage of special structures commonly found in model graphs. Part of the tick is to rearrange the graph to create the desired structure. These algorithms combined with conditional probabilities and evidence result in even large scale models becoming tractable.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elimination algorithms\n",
    "\n",
    "To understand the **elimination algorithm**, let's use an example based on a **chain graph**. Chain graphs occur in a wide range of applications including protein activation models. An example is shown in the figure below.\n",
    "\n",
    "<img src=\"img/Chain1.JPG\" alt=\"Drawing\" style=\"width:400px; height:75px\"/>\n",
    "<center> **Chain graph** </center>\n",
    "\n",
    "Our goal is to to compute the marginal distribution of $Z$, $P(Z)$:\n",
    "\n",
    "$$P(Z) = \\sum_V \\sum_W \\sum_X \\sum_Y P(V,W,X,Y,Z)$$\n",
    "\n",
    "We can decompose this distribution as follows:\n",
    "\n",
    "$$P(Z) = \\sum_V \\sum_W \\sum_X \\sum_Y P(V)\\ P(W \\ |\\ V)\\ P(X\\ |\\ W)\\ P(Y\\ |\\ X)\\ P(Z\\ |\\ Y)$$\n",
    "\n",
    "We can rearrange these terms as follows:\n",
    "\n",
    "$$P(Z) =  \\sum_W \\sum_X \\sum_Y P(X\\ |\\ W)\\ P(Y\\ |\\ X)\\ P(Z\\ |\\ Y) \\sum_V P(V)\\ P(W \\ |\\ V)$$\n",
    "\n",
    "Now:\n",
    "\n",
    "$$p(W) = \\sum_V P(V)\\ P(W \\ |\\ V)$$\n",
    "\n",
    "So we can rewrite the marginal distribution as:\n",
    "\n",
    "$$P(Z) =  \\sum_W \\sum_X \\sum_Y P(X\\ |\\ W)\\ P(Y\\ |\\ X)\\ P(Z\\ |\\ Y) p(W)$$\n",
    "\n",
    "We have **eliminated** $V$ from the graph as shown in the figure below. \n",
    "\n",
    "<img src=\"img/Eliminate1.JPG\" alt=\"Drawing\" style=\"width:400px; height:75px\"/>\n",
    "<center> **Eliminate V from the chain graph** </center>\n",
    "\n",
    "Only a **local cost** has been paid in this elimination. My local cost we mean that the summation was only over the variable $V$. \n",
    "\n",
    "We can continue the process by eliminating $W$ using local summation:\n",
    "\n",
    "$$P(Z) =  \\sum_X \\sum_Y P(Y\\ |\\ X)\\ P(Z\\ |\\ Y) \\sum_W p(W)\\ P(X\\ |\\ W) \\\\\n",
    "= \\sum_X \\sum_Y P(Y\\ |\\ X)\\ P(Z\\ |\\ Y)\\ p(X)$$\n",
    "\n",
    "<img src=\"img/Eliminate2.JPG\" alt=\"Drawing\" style=\"width:400px; height:75px\"/>\n",
    "<center> **Eliminate W from the chain graph** </center>\n",
    "\n",
    "Continuing the process $X$ is eliminated using local summation:\n",
    "\n",
    "$$P(Z) =  \\sum_Y P(Z\\ |\\ Y) \\sum_X p(X)\\ P(Y\\ |\\ X) \\\\\n",
    "= \\sum_Y P(Z\\ |\\ Y)\\ p(Y)$$\n",
    "\n",
    "\n",
    "<img src=\"img/Eliminate3.JPG\" alt=\"Drawing\" style=\"width:400px; height:75px\"/>\n",
    "<center> **Eliminate X from the chain graph** </center>\n",
    "\n",
    "Finally we can eliminate $Y$ using local summation to finally compute the marginal distribution of $Z$:\n",
    "\n",
    "$$P(Z) =  \\sum_Y p(Z)\\ P(Z\\ |\\ Y)$$\n",
    "\n",
    "<img src=\"img/Eliminate4.JPG\" alt=\"Drawing\" style=\"width:400px; height:75px\"/>\n",
    "<center> **Eliminate Y to compute the marginal distribution $P(Z)$** </center>  \n",
    "\n",
    "The complexity of this elimination process is $O(kn^2)$. This compares rather favorably with the NP problem of complexity of $O(n^k)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elimination on undirected chains  \n",
    "\n",
    "We can also apply elimination to **undirected chain graphs**. An example is shown in the figure below. \n",
    "\n",
    "<img src=\"img/Undirected1.JPG\" alt=\"Drawing\" style=\"width:400px; height:75px\"/>\n",
    "<center> **Undirected chain graph** </center>\n",
    "\n",
    "Our goal is to to compute the marginal distribution of $Z$, $P(Z)$. We can decompose this distribution as follows:\n",
    "\n",
    "$$P(Z) = \\sum_V \\sum_W \\sum_X \\sum_Y \\frac{1}{Z} \\phi(V,W)\\ \\phi(W,X)\\ \\phi(X,Y)\\ \\phi(Y,Z) \\\\\n",
    "= \\frac{1}{Z} \\sum_W \\sum_X \\sum_Y  \\phi(W,X)\\ phi(X,Y)\\ \\phi(Y,Z) \\sum_V \\phi(V,W) $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Message passing algorithms\n",
    "\n",
    "In this section we will see how the elimination algorithm can be generalized to **tree graphs** to create the **message passing** algorithm, also known as the **belief propagation** algorithm. \n",
    "\n",
    "Let's start with a simple example of a tree graph. \n",
    "\n",
    "> **Definition:** A tree graph is a undirected acyclic graph, in which any pair of nodes are connected by exactly one path. \n",
    "\n",
    "An example of a tree graph is shown in the figure below. Notice there is a flow of information from the bottom to the top. \n",
    "\n",
    "<img src=\"img/EliminationTree.JPG\" alt=\"Drawing\" style=\"width:350px; height:400px\"/>\n",
    "<center> **Elimination applied to a tree for query on node 1** </center>\n",
    "\n",
    "In general, we can compute the factor which results from eliminating variables below as follows:\n",
    "\n",
    "$$m_{ji}(x_i) = \\sum_{x_j} \\Big(\\ \\psi(x_j)\\ \\psi(x_i,x_j)\\ \\prod_{f \\in N(j) \\backslash i} m_{fj}(x_j) \\Big)$$\n",
    "\n",
    "> **Note:** The notation $f \\in N(j) \\backslash i$ indicates all factors $f$ in the set of parents $N(j)$ except $f = i$.\n",
    "\n",
    "We can also think of this formulation as **passing a message** from node $j$ to node $i$. In other words, we can stay that $m_{ji}(x_i)$ represents **propagating a \"belief\"** from node $j$ to node $i$. Notice that **elimination on a tree is equivalent to message passing along the branches of the tree**. \n",
    "\n",
    "We can summarize the message passing or belief propagation algorithm:\n",
    "\n",
    "> **Belief Propagation Algorithm**    \n",
    "```\n",
    "INPUT: Query node -> Q, graph tree    \n",
    "Root of tree = Q     \n",
    "FOR each edge from Q:    \n",
    "    Orient edge away from Q toward leaves   \n",
    "FOR message in schedule:   \n",
    "    WHILE NOT leaf node:    \n",
    "        propagate message in depth   \n",
    "        Perform elimination by message-passing, or Belief Propagation \n",
    "        ```\n",
    "\n",
    "\n",
    "> **Note:** Belief propagation uses the tree graph itself as the representation data structure. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing marginals with message passing\n",
    "\n",
    "Now that we have looked at the basics of the the message passing or belief propagation algorithm we will generalize the method to efficiently compute marginal distributions of the nodes of a tree graph. The naive approach would be to perform a query on each of the nodes in the graph. While this approach would work in principle, it is computationally inefficient. We will explore an algorithm that will efficiently compute the marginal distributions on the graph, including message reuse. \n",
    "\n",
    "A key fact to notice that a node only **sends a message to a neighbor only once it has received messages from all other neighbors**. For example, in the figure above the  messages must be passed in  the following order:\n",
    "\n",
    "1. $m_{53}x(x_3)$ and $m_{43}x(x_3)$,\n",
    "2. $m_{32}x(x_2)$, and\n",
    "3. $m_{21}x(x_1)$.\n",
    "\n",
    "If were where to continue with simple message passing to compute all the marginal distributions of the variables on the graph we would see that we would be recomputing the same messages several times. In fact, the computational complexity of this approach is $NC$ where $N$ is the number of nodes and $C$ is the complexity of the branching of the nodes. \n",
    "\n",
    "Keeping in mind that the ordering requirement guides the construction of efficient algorithms, we explore a method to extend the message passing algorithm. \n",
    "\n",
    "Developing an efficient method for computing the marginal distributions on a tree leads us to the **two pass algorithm**. The two pass algorithm proceeds by two steps:\n",
    "1. The **conditional probability distribution** (**CDP**) are updated using **Evidence**.  \n",
    "2. The nodes **collects** messages from their neighbors, which **emit** messages to the node. Leaf nodes emit messages at the start of the collection step. \n",
    "3. One a node has collected messages from neighbors it **distributes** messages to its neighbors. In the distribution phase, a node may only emit a messages once it has collected messages from all of its neighbors. \n",
    "4. The marginal distributions are computed. \n",
    "\n",
    "A schematic view of this algorithm is illustrated in the figure below. \n",
    "\n",
    "<img src=\"img/SumProductTree.JPG\" alt=\"Drawing\" style=\"width:450px; height:400px\"/>\n",
    "<center> **Two pass algorithm on a tree** </center>\n",
    "\n",
    "In the evidence step, the potential of nodes of evidence are updated using the following relationship:\n",
    "\n",
    "$$\\psi^E(x_i) = \\psi(x_i)\\ \\delta(x_i, \\bar{x}_j)\\\\\n",
    "where\\\\\n",
    "\\delta(x_i, \\bar{x}_j) = 1\\ if\\ i = j\\\\\n",
    "\\delta(x_i, \\bar{x}_j) = 0\\ otherwise$$\n",
    "\n",
    "Messages are computed for both the collection and distribution steps are computed using the aforementioned relationship:\n",
    "\n",
    "$$m_{ji}(x_i) = \\sum_{x_j} \\Big(\\ \\psi(x_j)\\ \\psi(x_i,x_j)\\ \\prod_{f \\in N(j) \\backslash i} m_{fj}(x_j) \\Big)$$\n",
    "\n",
    "The collection phase of the algorithm is illustrated in the figure below. \n",
    "\n",
    "<img src=\"img/Collect.JPG\" alt=\"Drawing\" style=\"width:450px; height:400px\"/>\n",
    "<center> **Collect phase of algorithm on a tree** </center>\n",
    "\n",
    "The distribute phase of the algorithm is shown in the figure below.\n",
    "\n",
    "<img src=\"img/Distribute.JPG\" alt=\"Drawing\" style=\"width:450px; height:400px\"/>\n",
    "<center> **Distribute phase of the algorithm on a tree** </center>\n",
    "\n",
    "Finally, the marginal distribution of the nodes are computed using the following relationship:\n",
    "\n",
    "$$p(x_i) = \\psi^E(x_i)\\ \\prod_{j \\in N(i)}m_{ji}(x_i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A computational example\n",
    "\n",
    "We have covered quite a bit of theory. Let's get practical and try a computational example. We will work on the student job application example. The DAG for this example is shown in the figure below.\n",
    "\n",
    "<img src=\"img/LetterDAG.JPG\" alt=\"Drawing\" style=\"width:400px; height:200px\"/>\n",
    "<center> **DAG for the student score and letter distribution** </center>\n",
    "\n",
    "Some of the inference methods in `pgmpy` rely on the `networkx` package. There are backward compatibility issues with the 2.X versions of `networkx`. If you do not have `networkx` version 1.11 installed, uncomment and execute the code in the cell below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -I networkx==1.11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pgmpy.models import BayesianModel\n",
    "from pgmpy.factors.discrete import TabularCPD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student_model = BayesianModel([('D', 'G'), ('I', 'G'), ('G', 'L'), ('I', 'S')])\n",
    "\n",
    "CDP_D = TabularCPD(variable='D', variable_card=2, values=[[0.7, 0.3]])\n",
    "CDP_I = TabularCPD(variable='I', variable_card=2, values=[[0.8, 0.2]])\n",
    "CDP_L = TabularCPD(variable='L', variable_card=2, \n",
    "                   values=[[0.1, 0.4, 0.99],\n",
    "                           [0.9, 0.6, 0.01]],\n",
    "                   evidence=['G'], # Leter depends on the grade\n",
    "                   evidence_card=[3])\n",
    "CDP_S = TabularCPD(variable='S', variable_card=2,\n",
    "                   values=[[0.95, 0.2],\n",
    "                           [0.05, 0.8]],\n",
    "                   evidence=['I'], # GRE score depneds on intelligence\n",
    "                   evidence_card=[2])\n",
    "CDP_G = TabularCPD(variable='G', variable_card=3, \n",
    "                   values=[[0.3, 0.05, 0.9,  0.5],\n",
    "                           [0.4, 0.25, 0.08, 0.3],\n",
    "                           [0.3, 0.7,  0.02, 0.2]],\n",
    "                  evidence=['I', 'D'],\n",
    "                  evidence_card=[2, 2])\n",
    "student_model.add_cpds(CDP_D, CDP_I, CDP_S, CDP_G, CDP_L)\n",
    "student_model.check_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pgmpy.inference import VariableElimination\n",
    "student_infer = VariableElimination(student_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student_infer.induced_width(['D', 'I', 'S', 'G', 'L'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╒═════╤══════════╕\n",
      "│ L   │   phi(L) │\n",
      "╞═════╪══════════╡\n",
      "│ L_0 │   0.7980 │\n",
      "├─────┼──────────┤\n",
      "│ L_1 │   0.2020 │\n",
      "╘═════╧══════════╛\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\StevePC2\\Anaconda3\\lib\\site-packages\\pgmpy\\factors\\discrete\\DiscreteFactor.py:598: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  phi1.values = phi1.values[slice_]\n"
     ]
    }
   ],
   "source": [
    "# Computing the probability of bronc given smoke.\n",
    "qur = student_infer.query(variables=['L'], evidence={'I':0, 'D':1})\n",
    "print(qur['L'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\StevePC2\\Anaconda3\\lib\\site-packages\\pgmpy\\factors\\discrete\\DiscreteFactor.py:586: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  phi.values = phi.values[slice_]\n"
     ]
    }
   ],
   "source": [
    "from pgmpy.inference import BeliefPropagation\n",
    "student_belief = BeliefPropagation(student_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\StevePC2\\Anaconda3\\lib\\site-packages\\pgmpy\\factors\\discrete\\DiscreteFactor.py:598: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  phi1.values = phi1.values[slice_]\n",
      "C:\\Users\\StevePC2\\Anaconda3\\lib\\site-packages\\pgmpy\\factors\\discrete\\DiscreteFactor.py:663: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  phi1.values = phi1.values[slice_]\n",
      "C:\\Users\\StevePC2\\Anaconda3\\lib\\site-packages\\pgmpy\\factors\\discrete\\DiscreteFactor.py:586: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  phi.values = phi.values[slice_]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'cardinality': {'D': 2, 'G': 3, 'I': 2, 'L': 2, 'S': 2},\n",
       " 'clique_beliefs': {('I',\n",
       "   'G',\n",
       "   'D'): <DiscreteFactor representing phi(I:2, G:3, D:2) at 0x2975db58470>,\n",
       "  ('I', 'S'): <DiscreteFactor representing phi(I:2, S:2) at 0x2975db58550>,\n",
       "  ('L', 'G'): <DiscreteFactor representing phi(L:2, G:3) at 0x2975db58978>},\n",
       " 'factors': defaultdict(list,\n",
       "             {'D': [<DiscreteFactor representing phi(D:2) at 0x2975d6fa9b0>,\n",
       "               <DiscreteFactor representing phi(G:3, I:2, D:2) at 0x2975d6fab38>],\n",
       "              'G': [<DiscreteFactor representing phi(G:3, I:2, D:2) at 0x2975d6fab38>,\n",
       "               <DiscreteFactor representing phi(L:2, G:3) at 0x2975d6fa908>],\n",
       "              'I': [<DiscreteFactor representing phi(G:3, I:2, D:2) at 0x2975d6fab38>,\n",
       "               <DiscreteFactor representing phi(I:2) at 0x2975d6fab70>,\n",
       "               <DiscreteFactor representing phi(S:2, I:2) at 0x2975bbe26d8>],\n",
       "              'L': [<DiscreteFactor representing phi(L:2, G:3) at 0x2975d6fa908>],\n",
       "              'S': [<DiscreteFactor representing phi(S:2, I:2) at 0x2975bbe26d8>]}),\n",
       " 'junction_tree': <pgmpy.models.JunctionTree.JunctionTree at 0x2975db58828>,\n",
       " 'model': <pgmpy.models.BayesianModel.BayesianModel at 0x2975db71be0>,\n",
       " 'sepset_beliefs': {frozenset({('I', 'G', 'D'),\n",
       "             ('I',\n",
       "              'S')}): <DiscreteFactor representing phi(I:2) at 0x2975db58748>,\n",
       "  frozenset({('I', 'G', 'D'),\n",
       "             ('L',\n",
       "              'G')}): <DiscreteFactor representing phi(G:3) at 0x2975db58860>},\n",
       " 'state_names': None,\n",
       " 'variables': ['D', 'G', 'I', 'L', 'S']}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student_belief.query(variables=['L','S'], evidence={'I':0, 'D':1})\n",
    "student_belief.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╒═════╤═════╤════════════╕\n",
      "│ L   │ G   │   phi(L,G) │\n",
      "╞═════╪═════╪════════════╡\n",
      "│ L_0 │ G_0 │     0.1000 │\n",
      "├─────┼─────┼────────────┤\n",
      "│ L_0 │ G_1 │     0.4000 │\n",
      "├─────┼─────┼────────────┤\n",
      "│ L_0 │ G_2 │     0.9900 │\n",
      "├─────┼─────┼────────────┤\n",
      "│ L_1 │ G_0 │     0.9000 │\n",
      "├─────┼─────┼────────────┤\n",
      "│ L_1 │ G_1 │     0.6000 │\n",
      "├─────┼─────┼────────────┤\n",
      "│ L_1 │ G_2 │     0.0100 │\n",
      "╘═════╧═════╧════════════╛\n",
      "╒═════╤═════╤════════════╕\n",
      "│ S   │ I   │   phi(S,I) │\n",
      "╞═════╪═════╪════════════╡\n",
      "│ S_0 │ I_0 │     0.9500 │\n",
      "├─────┼─────┼────────────┤\n",
      "│ S_0 │ I_1 │     0.2000 │\n",
      "├─────┼─────┼────────────┤\n",
      "│ S_1 │ I_0 │     0.0500 │\n",
      "├─────┼─────┼────────────┤\n",
      "│ S_1 │ I_1 │     0.8000 │\n",
      "╘═════╧═════╧════════════╛\n"
     ]
    }
   ],
   "source": [
    "print(student_belief.factors['L'][0])\n",
    "print(student_belief.factors['S'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('I', 'S'): <DiscreteFactor representing phi(I:2, S:2) at 0x2975db58550>, ('I', 'G', 'D'): <DiscreteFactor representing phi(I:2, G:3, D:2) at 0x2975db58470>, ('L', 'G'): <DiscreteFactor representing phi(L:2, G:3) at 0x2975db58978>}\n",
      "╒═════╤═════╤════════════╕\n",
      "│ L   │ G   │   phi(L,G) │\n",
      "╞═════╪═════╪════════════╡\n",
      "│ L_0 │ G_0 │     0.0336 │\n",
      "├─────┼─────┼────────────┤\n",
      "│ L_0 │ G_1 │     0.1253 │\n",
      "├─────┼─────┼────────────┤\n",
      "│ L_0 │ G_2 │     0.3473 │\n",
      "├─────┼─────┼────────────┤\n",
      "│ L_1 │ G_0 │     0.3024 │\n",
      "├─────┼─────┼────────────┤\n",
      "│ L_1 │ G_1 │     0.1879 │\n",
      "├─────┼─────┼────────────┤\n",
      "│ L_1 │ G_2 │     0.0035 │\n",
      "╘═════╧═════╧════════════╛\n",
      "╒═════╤═════╤════════════╕\n",
      "│ I   │ S   │   phi(I,S) │\n",
      "╞═════╪═════╪════════════╡\n",
      "│ I_0 │ S_0 │     0.7600 │\n",
      "├─────┼─────┼────────────┤\n",
      "│ I_0 │ S_1 │     0.0400 │\n",
      "├─────┼─────┼────────────┤\n",
      "│ I_1 │ S_0 │     0.0400 │\n",
      "├─────┼─────┼────────────┤\n",
      "│ I_1 │ S_1 │     0.1600 │\n",
      "╘═════╧═════╧════════════╛\n"
     ]
    }
   ],
   "source": [
    "print(student_belief.clique_beliefs)\n",
    "print(student_belief.clique_beliefs['L', 'G'])\n",
    "print(student_belief.clique_beliefs['I', 'S'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Factor graphs\n",
    "\n",
    "Several efficient inference methods make use of **factor graphs**. A factor graph can be undirected on directed. Factor graphs can have advantages over both DAGs and Markov networks in terms of the independencies which can be represented.   \n",
    "\n",
    "**Definition:** A factor graph is comprised of factors $\\phi_i(\\mathcal{X_i})$. A function $f(x_1, x_2, \\ldots, x_n)$, is represented on a graph with an undirected link on the each variable $x_i$, and $x_j \\in \\mathcal{X_i}$.  The function is then represented as:\n",
    "\n",
    "$$f(x_1, x_2, \\ldots, x_n) = \\prod_{i=1}^n \\phi_i(\\mathcal{X_i})$$\n",
    "\n",
    "In other words, a factor graph is a representation of a set of variables with a factor $\\phi_i(\\mathcal{X_i})$ on each edge. \n",
    "\n",
    "We can use the above formulation to represent a distribution as follows:\n",
    "\n",
    "$$p(x_1, x_2, \\ldots, x_n) = \\frac{1}{Z} \\prod_{i=1}^n \\phi_i(\\mathcal{X_i}) \\\\\n",
    "where\\\\\n",
    "Z = \\sum_{\\mathcal{X}} \\prod_{i=1}^n \\phi_i(\\mathcal{X_i})$$\n",
    "\n",
    "Let's look at an example of a undirected factor graph. The Markov network and the corresponding factor graphs are shown in the figure below. \n",
    "\n",
    "<img src=\"img/MarkovFactor.JPG\" alt=\"Drawing\" style=\"width:400px; height:150px\"/>\n",
    "<center> **Markov Network and Factor Graphs** </center>\n",
    "\n",
    "The Markov network shown on the left has a single clique. This leads to two possible factorizations. The factor graph in the middle can represent the distribution with a single click= potential $\\psi(X,Y,Z)$.  The factor graph on the right represents another factorization of the distribution into multiple clique potentials:\n",
    "\n",
    "$$p(X,Y,Z) = \\psi(X,Y) \\psi(X,Z) \\psi(Z,Y)$$\n",
    "\n",
    "These two factorizations represent different factorizations with different independencies. But, they have the same representation on the Markov network. \n",
    "\n",
    "Let's look at another example. The figure below shows a simple directed network (DAG) and the corresponding factor graph. \n",
    "\n",
    "<img src=\"img/DAGFactor.JPG\" alt=\"Drawing\" style=\"width:250px; height:200px\"/>\n",
    "<center> **DAG and Corresponding Factor Graph** </center>\n",
    "\n",
    "The factors required to represent a DAG are considerably different from those required for an undirected graph. The leaf nodes of the graph have terminal factors. Such factors are required to represent the distribution of the original DAG. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sum-product algorithm\n",
    "\n",
    "The generalization of the elimination method is known as the **sum-product algorithm**. The sum-product algorithm operates as messages on a factor graph. \n",
    "\n",
    "The sum-product algorithm is built on two message constructs, **variable to factor messages** and **factor to variable messages**. Let's start with the variable to factor message which is illustrated in the figure below. \n",
    "\n",
    "<img src=\"img/VarToFactor.JPG\" alt=\"Drawing\" style=\"width:350px; height:200px\"/>\n",
    "<center> **Variable to factor message example** </center>\n",
    "\n",
    "In this figure the message passed from the variable $x$ to the factor $f$ is the product of the messages the variable receives from other factors $\\{ f_1, f_2, f_3 \\}$ and can be expressed as:\n",
    "\n",
    "$$u_{x \\rightarrow f}(x) = \\prod_{h \\in na(x) \\backslash f} u_{h \\rightarrow x}(x)$$  \n",
    "\n",
    "In words potential of variable $x$ is product of the potentials of all neighboring factors, $na(x)$, except the factor to which the message is being being passed, denoted $\\backslash f$. \n",
    "\n",
    "The factor to variable message is illustrated in the figure below:\n",
    "\n",
    "<img src=\"img/FactorToVar.JPG\" alt=\"Drawing\" style=\"width:350px; height:200px\"/>\n",
    "<center> **Factor to variable message example** </center>\n",
    "\n",
    "In this case, the potential of the factor, $f$, is passed to the variable $x$. The potential is the summation over all states, $\\chi$, of the variable except $x$ of the product all messages the factor receives from its neighbors, $na(f)$, except $x$, denoted $\\backslash x$. This potential can be expressed as:\n",
    "\n",
    "$$u_{f \\rightarrow x}(x) = \\sum_{\\chi_f \\backslash x} \\psi_f(\\chi_f) \\prod_{y \\in na(f) \\backslash x} u_{y \\rightarrow f}(y)$$\n",
    "\n",
    "\n",
    "Let's make this a bit more concrete. Ultimately, we want to compute the **marginal distribution** over the states $\\chi_f$. This can problem can be formulated as:\n",
    "\n",
    "$$p(\\chi) = \\frac{1}{Z} \\prod_f \\psi_f(\\chi_f)\\\\\n",
    "where\\\\\n",
    "Z = \\sum_\\chi \\frac{1}{Z} \\prod_f \\psi_f(\\chi_f)$$\n",
    "\n",
    "\n",
    "The sum-product algorithm can be summarized as follows:\n",
    "\n",
    "> **Sum-Product Algorithm**\n",
    "```\n",
    "Input: query_node = x1, tree\n",
    "Sort tree with x1 last.\n",
    "Place potentials on the active list\n",
    "FOR each node:\n",
    "    Eliminate ith node by taking the sum-product over all neighbor potentials.\n",
    "    Place the resulting factor on the active list\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Max-product algorithm\n",
    "\n",
    "In many applications such as decision problems and control problems, we really just want to know the **maximum a posteriori** or **MAP** for the marginal distribution of interest. A simple modification of the sum-product algorithm, known as the **max-product algorithm**, gives us a relatively efficient method to compute the MAP. This method is also sometimes known as the **belief revision algorithm**.\n",
    "\n",
    "The variable to factor message for the max-product algorithm is identical to the sum-product algorithm.\n",
    "\n",
    "$$u_{x \\rightarrow f}(x) = \\prod_{h \\in na(x) \\backslash f} u_{h \\rightarrow x}(x)$$  \n",
    "\n",
    "The for the factor to variable message the max-product algorithm only requires that we take the maximum:\n",
    "\n",
    "$$u_{f \\rightarrow x}(x) = argmax_{\\chi_f \\backslash x} \\psi_f(\\chi_f) \\prod_{y \\in na(f) \\backslash x} u_{y \\rightarrow f}(y)$$\n",
    "\n",
    "The marginal then becomes just:\n",
    "\n",
    "$$p(\\chi) = argmax_x \\prod_{f \\in ne x} u_{f \\rightarrow x}(x)$$\n",
    "\n",
    "You can find more details of the max-product algorithm in the suggested readings or Sections 13.1, 13.2 and 13.3 of Kholer and Freeman.  \n",
    "\n",
    "There are other variations on the sum-product algorithm which can be quite useful in applications. For example, we may want to know the N most probable states. Or, we may want to find the shortest route or path though the tree of random variables. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to the junction tree algorithm\n",
    "\n",
    "In the previous section we examined the message passing or belief propagation algorithm. This algorithm can efficiently compute the marginal distributions of variables in a tree graph. \n",
    "\n",
    "There is another way to create an efficient message passing algorithm. This method is known as the **junction tree algorithm**. The junction tree algorithm can factorize graphs that do not have a tree structure initially. Thus, the junction tree algorithm is not only efficient, but quite flexible. \n",
    "\n",
    "The general steps to of the junction tree algorithm are:\n",
    "1. Moralize the graph, following the process we have already applied.\n",
    "2. Triangulate the graph to transform multiply connected graphs to trees. \n",
    "3. Build a clique tree from the transformed graph. The clique tree is composed of clique variables and factors. \n",
    "4. Propagate the probabilities by local message passing from either factors to variables or variables and factors. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Junction tree for multiply connected graph\n",
    "\n",
    "Let's work though a more complex example for a multiply connected graph. A example is shown in the figure below.\n",
    "\n",
    "<img src=\"img/MultiConnected.JPG\" alt=\"Drawing\" style=\"width:350px; height:275px\"/>\n",
    "<center> **A multiply connected graph** </center>\n",
    "\n",
    "This graph is clearly not a tree. There are multiple paths between many nodes. The question is, how can we transform the graph so that we can perform message passing?\n",
    "\n",
    "### Moralization\n",
    "\n",
    "The graph illustrated above has several immoralities. However, the moralizatoin process for this graph is straight-forward. The result is shown in the figure below.  \n",
    "\n",
    "<img src=\"img/MultiConnectedMoralized.JPG\" alt=\"Drawing\" style=\"width:350px; height:275px\"/>\n",
    "<center> **Moralized multiply connected graph** </center>\n",
    "\n",
    "There is a problem with this graph. There are several **cycles** with four variables. \n",
    "\n",
    "### Triangularization \n",
    "\n",
    "With the graph moralized, you can see that there are cycles with four or more variables. The question at this point is if the cliques created in these cycles lead to consistent results. To purse this question consider the graph and the resulting clique tree shown in the figure below.\n",
    "\n",
    "<img src=\"img/FourCycle.JPG\" alt=\"Drawing\" style=\"width:350px; height:150px\"/>\n",
    "<center> **A four cycle graph with the corresponding clique tree** </center>\n",
    "\n",
    "Notice that there is no way to ensure that the probability associated with variable 3 is consistent between the two branches of the tree. There is no guarantee of **global consistency** with cycles of four or more variables. \n",
    "\n",
    "To solve the above problem we need to perform a procedure known as **triangularization**. The triangularization procedure adds an edge to the four cycle as shown in the figure below.  \n",
    "\n",
    "<img src=\"img/Triangle1.JPG\" alt=\"Drawing\" style=\"width:350px; height:300px\"/>\n",
    "<center> **A triangleized four cycle graph with the corresponding clique trees** </center>\n",
    "\n",
    "The triangularized graph leads to a globally consistent clique graph. Notice that there are two possible triangularizations, which lead to two different clique trees. While the clique trees are globally consistent there they are not unique. \n",
    "\n",
    "<img src=\"img/Triangle2.JPG\" alt=\"Drawing\" style=\"width:350px; height:275px\"/>\n",
    "<center> **A triangleized multiply connected graph** </center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pgmpy.models import JunctionTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No sepset found between these two edges.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-0b02db51639f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mstudent_gt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mJunctionTree\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mstudent_gt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_node\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'I'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'D'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'S'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'G'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'L'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mstudent_gt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_edges_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'I'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'S'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'D'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'I'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'G'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#, ('I','G')]) #, ('D','G'), ('G','L')])\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pgmpy\\base\\UndirectedGraph.py\u001b[0m in \u001b[0;36madd_edges_from\u001b[1;34m(self, ebunch, weights)\u001b[0m\n\u001b[0;32m    242\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    243\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0medge\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mebunch\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 244\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_edge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0medge\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0medge\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    245\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    246\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mis_clique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnodes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pgmpy\\models\\JunctionTree.py\u001b[0m in \u001b[0;36madd_edge\u001b[1;34m(self, u, v, **kwargs)\u001b[0m\n\u001b[0;32m     75\u001b[0m                              'properties of Junction Tree'.format(u=str(u), v=str(v)))\n\u001b[0;32m     76\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 77\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mJunctionTree\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_edge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     78\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcheck_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pgmpy\\models\\ClusterGraph.py\u001b[0m in \u001b[0;36madd_edge\u001b[1;34m(self, u, v, **kwargs)\u001b[0m\n\u001b[0;32m    121\u001b[0m         \u001b[0mset_v\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mset_u\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misdisjoint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset_v\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 123\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'No sepset found between these two edges.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    124\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mClusterGraph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_edge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: No sepset found between these two edges."
     ]
    }
   ],
   "source": [
    "student_gt = JunctionTree()\n",
    "student_gt.add_node(('I', 'D', 'S', 'G', 'L'))\n",
    "student_gt.add_edges_from([('I','S'), ('D', 'I', 'G')]) #, ('I','G')]) #, ('D','G'), ('G','L')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Copyright 2018, Stephen F Elston. All rights reserved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
