{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approximate Inference Algorithms\n",
    "\n",
    "\n",
    "\n",
    "## CSCI E-83\n",
    "## Stephen Elston\n",
    "\n",
    "\n",
    "In the previous lesson we examined several exact inference methods. While factoring the distribution and restructuring the graph reduces the computational complexity there are limits. Exact inference methods are **tabular** in nature. A tabular method must perform the full set of matrix multiplications on the messages. \n",
    "\n",
    "For many real-world applications, tabular methods are simply infeasible. In this lesson we will focus on **approximate inference algorithms**. Approximate inference algorithms operate by **random sampling**. The posterior distribution is approximated by the samples.   \n",
    "\n",
    "For Bayesian networks, approximate inference or learning is used in two ways:  \n",
    "1. **Parameter estimation** where the parameters of a model are estimated. The model might be .............  \n",
    "2. **Structure estimation** is the process of estimating the structure of the graphical model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exact tabular methods vs. approximations\n",
    "\n",
    "In the previous lesson we examined exact inference methods. These methods are applied primarily to tabular data. In tabular methods every entry in each CDP table must be propagated in the messages. The computational complexity of these methods can be a serious limitations on scalability.  \n",
    "\n",
    "In this lesson we will explore approximate inference methods. Approximate methods are generally much more scalable than exact methods. As a result, approximate methods are used in large scale solutions.      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple parameter estimation example\n",
    "\n",
    "In previous lessons we used a belief network of a student looking for a job. As you will recall the student must submit her GRE score, $S$ and her letter of recommendation, $L$. Said that the probability of the GRE score (high, low) was conditional on the student's intelligence, $I$. We had a prior distribution of $I = [0.8, 0.2] = [p(high), p(low)]$. But, how do we estimate the parameters for this distribution?\n",
    "\n",
    "There are several methods\n",
    "\n",
    "### Maximum likelihood method\n",
    "\n",
    "### Bayesian inference\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Copyright 2018 Stephen F Elston. All rights reserved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
