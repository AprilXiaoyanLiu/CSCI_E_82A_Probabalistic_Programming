{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Learning Model Parmaters\n",
    "\n",
    "\n",
    "\n",
    "## CSCI E-83\n",
    "## Stephen Elston\n",
    "\n",
    "\n",
    "In the previous lesson we examined several exact inference methods. When we applied these methods inference methods the parameters of the conditional probability distributions (CPD) were assumed known. But, how do we find these parameters in practice? \n",
    "\n",
    "In this lesson we will focus on the **learning** for Bayesian network models. There are two aspects of learning for Bayesian networks:  \n",
    "1. **Parameter estimation** where the parameters of the CPDs of a model are estimated. We focus on this aspect of the problem in this notebook.   \n",
    "2. **Structure estimation** is the process of estimating the structure of the graphical model. This problem is the subject of another notebook.  \n",
    "\n",
    "The role of learning in an intelligent agent is illustrated in the figure below. For Bayesian models the learning and inference processes often merge as in the case where evidence is applied.  \n",
    "\n",
    "<img src=\"img/Learning.JPG\" alt=\"Drawing\" style=\"width:400px; height:200px\"/>\n",
    "<center> **Learning in an intelligent agent** </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple parameter estimation example\n",
    "\n",
    "In previous lessons we used a belief network of a student looking for a job. As you will recall the student must submit her GRE score, $S$ and her letter of recommendation, $L$. Said that the probability of the GRE score (high, low) was conditional on the student's intelligence, $I$. We had a prior distribution of $I = [0.8, 0.2] = [p(high), p(low)]$. But, how do we estimate the parameters for this distribution?\n",
    "\n",
    "There are several methods which can be used to estimate the parameters of a distribution including:\n",
    "- Frequentist **maximum likelihood** or **ML** methods which only require that a likelihood function be specified. \n",
    "- Bayesian **maximum a-postiori** or **MAP** methods, which require the specification of both a likelihood function and a prior distribution of the model parameters. \n",
    "\n",
    "In this lesson we will focus on the MAP methods, but will also investigate ML methods. MAP methods are inherently Bayesian and therefore provide a more complete picture of distributions. However, MAP methods can be difficult to implement and computationally intensive. Thus in practice, ML methods are used. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### The Bernoulli and Beta distributions\n",
    "\n",
    "For this example, we will work with a binary variable with states {0,1}. A **single realization** of such a variable has a **Bernoulli distribution**. The Bernoulli distribution is a **parametric distribution** which we can express as:\n",
    "\n",
    "$$p(\\nu = 1) = \\Theta \\\\\n",
    "where\\\\\n",
    "\\nu = an\\ observation\\\\\n",
    "\\Theta = probability\\ parameter$$\n",
    "\n",
    "For a series of observations of a binary valued variable we use the **Binomial distribution**. We call each observation where $\\nu = 1$ a **success**. The Binomial distribution of $k$ successes in $n$ trials is then expressed as:\n",
    "\n",
    "$$p(\\nu = k\\ |\\ \\Theta) = \\binom{n}{k} \\Theta^k (1-\\Theta)^{n-k}$$\n",
    "\n",
    "We want to specify a **Binomial likelihood** in Bayes theorem, we will also need to specify a prior distribution. We pick a prior distribution which is **conjugate** to the likelihood distribution so that the posterior distribution is the same as the prior. We can express this concept as:\n",
    "\n",
    "$$posterior\\_distribution(hypothesis\\ |\\ data) = \\frac{likelihood(data\\ |\\ hypothesis)* conjugate\\_prior(hypothesis)}{marginal\\ distribtion\\ data }\\\\\n",
    "where\\\\\n",
    "posterior\\_distribution\\ is\\ same\\ family\\ as\\ prior\\_distribution$$\n",
    "\n",
    "The question is, which distribution is the conjugate to the Binomial distribution? The answer is the **Beta distribution**. We can express the Beta distribution as: \n",
    "\n",
    "$$p(x\\ |\\ \\alpha, \\beta) = \\frac{1}{B(\\alpha, \\beta)}x^{\\alpha - 1}(1-x)^{\\beta - 1},\\ 0 \\le x \\le 1\\\\\n",
    "where\\\\\n",
    "B(\\alpha, \\beta) = \\frac{\\Gamma(\\alpha) \\Gamma(\\beta)}{\\Gamma(\\alpha,\\beta)}\\\\\n",
    "and\\\\\n",
    "\\Gamma(x) = Gamma\\ function$$\n",
    "\n",
    "At first glance, the Beta distribution is a bit complicated. But for the most part the normalization with the Gamma function are not terribly important. \n",
    "\n",
    "You can develop some intuition about the Beta distribution by plotting it for various values of $\\alpha$ and $\\beta$. Execute the code in the cell below and examine the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import itertools as it\n",
    "import scipy.stats as ss\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "alpha = [.5, 1, 2, 3, 4]\n",
    "beta = alpha[:]\n",
    "x = np.linspace(.001, .999, num=100)\n",
    "\n",
    "for i, (a, b) in enumerate(it.product(alpha, beta)):\n",
    "    plt.subplot(len(alpha), len(beta), i+1)\n",
    "    plt.plot(x, ss.beta.pdf(x, a, b))\n",
    "    plt.title('(a,b) = (%d,%d)' % (a,b))\n",
    "plt.tight_layout()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some points to notice here:\n",
    "\n",
    "- The range is $0 \\le Beta(x) \\le 1$.\n",
    "- If $\\alpha \\gt \\beta$ the distribution is left skewed, if $\\alpha \\lt \\beta$ the distribution is right skewed, and if $\\alpha = \\beta$ the distribution is symmetric.\n",
    "- For $\\alpha = 1$, $\\beta = 1$ the distribution is uniform. \n",
    "\n",
    "With these preliminary in mind, we will now look at methods to estimate the parameter of the Binomial likelihood distribution $\\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum likelihood method\n",
    "\n",
    "We will not go into the details of **maximum likelihood estimation** (**MLE**). These details can be found in many standard texts on statistics and machine learning including Murphy (Section 7.3), Hastie, Tibshirani and Friedman (Section 2.6), and many others.  \n",
    "\n",
    "\n",
    "Not surprisingly, in order to perform maximum likelihood estimation, we must define a **likelihood function** for the distribution we are trying to estimate. Given data $\\mathcal{V}$ and parameters of the distribution, $\\Theta$, we can express a likelihood function as $\\mathcal{L}(\\Theta, \\mathcal{V}) = p(\\mathcal{V}\\ |\\ \\Theta)$. In most cases, we will work with the **log likelihood**, $log(p(\\mathcal{V}\\ |\\ \\Theta))$. The maximum likelihood estimate or MLE for the parameters is then:\n",
    "\n",
    "$$\\hat{\\Theta} \\doteq argmax_{\\Theta} \\{ log(p(\\mathcal{V}\\ |\\ \\Theta)) \\}$$\n",
    "\n",
    "In words, we want to find the parameters which maximize the likelihood of the distribution for the data sample. \n",
    "\n",
    "Let's use computing the maximum likelihood for the Bernoulli distribution as an example. The likelihood function for the Bernoulli distribution is given a datat vector $\\mathcal{V}$:\n",
    "\n",
    "$$L(p) = \\prod_{i=1}^n p^{\\nu_i}\\ (1-p)^{(1 - \\nu_i)})$$\n",
    "\n",
    "The log likelihood is easily computed:\n",
    "\n",
    "$$\\mathcal{l}(p) = log(p) \\sum_{i=1}^n \\nu_i + log(1-p)  \\sum_{i=1}^n (1 - \\nu_i)$$\n",
    "\n",
    "To find the maximum of this function we must find a point where the first partial derivative is zero:\n",
    "\n",
    "$$\\frac{\\partial \\mathcal{l}(p)}{\\partial p} = \\frac{\\sum_{i=1}^n \\nu_i}{p} \n",
    "- \\frac{\\sum_{i=1}^n (1 - \\nu_i)}{1-p} = 0$$\n",
    "\n",
    "The solution is: \n",
    "\n",
    "$$p = \\frac{1}{n} \\sum_{i=1}^n \\nu_i$$\n",
    "\n",
    "The simple result is that the maximum likelihood value of $p$ is just the mean of the data. \n",
    "\n",
    "You can verify that the second partial derivative is negative, indicating this is the maximum of the log likelihood function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian learning\n",
    "\n",
    "We can now try the Bayesian approach to estimating the parameter $\\Theta$. Conceptually you can visualize this process as a directed Bayesian model as shown below. \n",
    "\n",
    "<img src=\"img/PlateDiagram.JPG\" alt=\"Drawing\" style=\"width:400px; height:200px\"/>\n",
    "<center> **Bayes network and plate diagram for parameter estimation** </center>\n",
    "\n",
    "The parameter $\\Theta$ is causal to the vector of data values $\\{ \\nu^1, \\nu^2, \\nu^3, \\ldots \\nu^N \\}$. This relationship is shown as a DAG on the left side of the figure above. On the right side of the diagram is a summary using **plate notation**. The plate is short hand for the $N$ values. \n",
    "\n",
    "There \n",
    "\n",
    "$$p(\\Theta\\ |\\ \\nu^1, \\nu^2, \\nu^3, \\ldots \\nu^N) \\propto p(\\Theta) \\prod_{n=1}^N p(\\nu^n\\ |\\ \\Theta)\n",
    "= p(\\Theta) \\prod_{n=1}^N |Theta^{I(\\nu^n=1)}(1 - \\Theta)^{I(\\nu^n=0)}\\\\\n",
    "\\propto p(\\Theta)\\ \\Theta^{\\sum_{n=1}^N I(\\nu^n=1)}(1 - \\Theta)^{\\sum_{n=1}^N I(\\nu^n=0)} $$\n",
    "\n",
    "For a simple binary case of $\\{ True, False \\}$ the solution can be obtained with the prior distribution $p(\\Theta)$ and the counts, $\\{ N_T, N_F \\}$.\n",
    "\n",
    "$$p(\\Theta\\ |\\ \\nu^1, \\nu^2, \\nu^3, \\ldots \\nu^N) \\propto p(\\Theta)\\ \\Theta^{N_T} (1 - \\Theta)^{N_F}$$  \n",
    "Given a data vector, $\\mathcal{V}$, how do we compute the most probable value of $\\Theta$. The solution in this case is realitively easy and can be performed using just counts of the two values, {True, False} or {1,0} and the prior distribution. \n",
    "\n",
    "The likelihood given counts can be expressed as follows.\n",
    "\n",
    "$$\\Theta^{\\mathcal{C}(\\nu_i = 1)} (1-\\Theta)^{\\mathcal{C}(\\nu_i = 0)}\\\\\n",
    "where\\\\\n",
    "\\mathcal{C} = count\\ operator$$\n",
    "\n",
    "Using the above likelihood, the posterior distribution can be expressed as:\n",
    "\n",
    "$$p(\\Theta\\ |\\ \\mathcal{V}) \\propto p(\\Theta)\\ \\Theta^{\\mathcal{C}(\\nu_i = 1)} (1-\\Theta)^{\\mathcal{C}(\\nu_i = 0)}$$\n",
    "\n",
    "Further details can be found in Murphy, Section 3.3 or Barber, Section 9.4. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computational Example\n",
    "\n",
    "Now, its time to put all this theory into practice with a computational example. We will continue to work with the student example illustrated below:\n",
    "\n",
    "<img src=\"img/LetterDAG.JPG\" alt=\"Drawing\" style=\"width:400px; height:200px\"/>\n",
    "<center> **DAG CPDs for student example** </center>\n",
    "\n",
    "In this example we will perform the following:\n",
    "\n",
    "1. Simulate data for the CPDs of the model.\n",
    "2. Test the CDPs by estimating the parameters using both maximum likelihood and maximum a posteriori methods.\n",
    "3. Estimate the parameters for the entire model using both MLE and MAP methods. We will be able to examine the effects of different prior distributions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulation of CPDs\n",
    "\n",
    "Creating, testing and debugging simulation software can be tricky. Some of the techniques which can make your life easier are the same as you should use when developing any analytics software, or even software in general. These techniques include:\n",
    "\n",
    "- Build your simulation as a series of small, easily tested chunks In practice, this means you will build your simulation by creating and testing a set of small functions that comprise the overall model.\n",
    "- Test each small functional unit individually. These tests should include at least testing some typical cases, as well as boundary or extreme cases. Sensible behavior with extreme or limiting cases is a requirement for a stable simulation. Both tabular and graphical output can be useful for evaluating tests.\n",
    "- Test your overall simulation each time you add a new functional component. This processes ensures that all the pieces work together. \n",
    "- Simulations are inherently stochastic. If you want to create identical numerical results, say for automated testing, set a seed before you begin tests. In this notebook no seed is set so you can experience the stochastic nature of the simulation. \n",
    "\n",
    "The code in the cell below computes the CPD of D, the difficulty of the course. This part of the simulation is easy, as the samples are just realizations of a Bernoulli distribution. Setting the first parameter of the numpy `binomial` function to 1 computes the result of a single trial, which is a realization of the Bernoulli distribution. There a probability of 0.7 that the machine learning course is difficult. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Simulate the binary tables\n",
    "import numpy as np\n",
    "import numpy.random as nr\n",
    "import pandas as pd\n",
    "\n",
    "nsamp = 25\n",
    "nr.seed(234)\n",
    "D_samps = nr.binomial(1, 0.7, nsamp)\n",
    "D_samps = pd.DataFrame(D_samps, columns = ['D'])\n",
    "D_samps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of above simulation looks reasonable. \n",
    "\n",
    "### MLE\n",
    "\n",
    "But, what happens when we try to estimate the parameters of the distribution? The code in the cell below does the following:\n",
    "\n",
    "1. Constructs a DAG with a single node.\n",
    "2. Performs MLE for the model parameters and displays the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the Bayesian Model\n",
    "from pgmpy.models import BayesianModel\n",
    "from pgmpy.estimators import MaximumLikelihoodEstimator, BayesianEstimator\n",
    "\n",
    "test_model = BayesianModel()\n",
    "test_model.add_node('D')\n",
    "\n",
    "# Fitting the data to the model using Maximum Likelihood Estimator\n",
    "test_model.fit(D_samps, estimator=MaximumLikelihoodEstimator)\n",
    "print(test_model.get_cpds('D'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MLE of the parameters are close to, but not exactly the same as, the value we used for the simulation. \n",
    "\n",
    "### MAP\n",
    "\n",
    "Next, we will try a Bayesian estimator. A Dirichlet distribution is used as the prior. We will examine the properties of this distribution in more detail shortly. \n",
    "\n",
    "The prior of a Dirichlet distribution is specified as a set of **pseudo counts**. As with any Bayesian estimation problem, we must face the issue of specification of the prior distribution. For this example, we could use any one, or perhaps a combination of the following approaches:\n",
    "1. Select pseudo counts from previous data sets for the same or similar problems. \n",
    "2. Apply expert judgment. \n",
    "3. Use an uninformative prior, such as a uniform distribution, in which case the pseudo counts are equal. \n",
    "\n",
    "The code in the cell below uses the last approach, a uniform distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the data to the model using Maximum Likelihood Estimator\n",
    "pseudo_counts = {'D': [20, 20]}\n",
    "test_model.fit(D_samps, estimator=BayesianEstimator, prior_type='dirichlet', pseudo_counts=pseudo_counts)\n",
    "print(test_model.get_cpds('D'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MAP estimate of parameters are closer to 0.5. This is expected given the uniform prior. \n",
    "\n",
    "### Simulation of Another CPD\n",
    "\n",
    "There is one more unconditional distribution in our model, I, the intelligence of the student. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nr.seed(1234)\n",
    "I_samps = pd.DataFrame(nr.binomial(1, 0.8, nsamp), columns = ['I'])\n",
    "print(I_samps)\n",
    "\n",
    "## Estimate the model parameters as a test\n",
    "test_model = BayesianModel()\n",
    "test_model.add_node('I')\n",
    "\n",
    "# Fitting the data to the model using Maximum Likelihood Estimator\n",
    "test_model.fit(I_samps, estimator=MaximumLikelihoodEstimator)\n",
    "print(test_model.get_cpds('I'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As, with the first conditionally independent parameter, these estimates are close, but not exactly the same as the simulation. \n",
    "\n",
    "### Simulating Conditionally Dependent Distributions\n",
    "\n",
    "Up to this point, we have only simulated conditionally independent distributions. Now, we will simulate a conditionally dependent distribution, S, the student's GRE score.  This distribution is dependent only on the student's intelligence, I, the parent variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim_bernoulli(p, n = 25):\n",
    "    \"\"\"\n",
    "    Function to compute the vectors with probabilities for each \n",
    "    condition (input value) of the dependent variable using the Bernoulli\n",
    "    distribution. \n",
    "    \n",
    "    The arguments are:\n",
    "    p - a vector of probabilites of success for each case.\n",
    "    n - The numer of realizations. \n",
    "    \"\"\"\n",
    "    temp = np.zeros(shape = (len(p), n))\n",
    "    for i in range(len(p)): \n",
    "        temp[i,:] = nr.binomial(1, p[i], n)\n",
    "    return(temp)\n",
    "\n",
    "def selec_dist_1(sims, var, lg):\n",
    "    \"\"\"\n",
    "    Function to integrate the conditional probabilities for\n",
    "    each of the cases of the parent variable. \n",
    "    \n",
    "    The arguments are:\n",
    "    sims - the array of simulated realizations with one row for each state of the\n",
    "           parent variable. \n",
    "    var - the vector of values of parent variable used to select the value from the \n",
    "          sims array.\n",
    "    lg - vector of states of possible states of the parent variable. These must be\n",
    "         in the same order as for the sims array. \n",
    "    \"\"\"\n",
    "    out = sims[0,:] # Copy of values for first parent state\n",
    "    var = np.array(var).ravel()\n",
    "    for i in range(1, sims.shape[0]): # loop over other parent states\n",
    "        out = [x if u == lg[i] else y for x,y,u in zip(sims[i,:], out, var)]\n",
    "    return([int(x) for x in out])\n",
    "\n",
    "## Specify the vector of success probabilites and \n",
    "probs = [0.2, 0.8]\n",
    "bern = sim_bernoulli(probs)\n",
    "\n",
    "nr.seed(2345)\n",
    "S_samps = pd.DataFrame(selec_dist_1(bern, I_samps, [0,1]), columns = ['S'])\n",
    "print(S_samps)\n",
    "\n",
    "test_model = BayesianModel()\n",
    "test_model.add_node('S')\n",
    "\n",
    "# Fitting the data to the model using Maximum Likelihood Estimator\n",
    "test_model.fit(S_samps, estimator=MaximumLikelihoodEstimator)\n",
    "print(test_model.get_cpds('S'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with the conditionally independent distributions the parameters are close to, but not the same, as the probabilities used for the simulation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global and Local Parameter Independence\n",
    "\n",
    "In the foregoing we have only considered the univariate Binomial and Beta distributions. But, real-world problems have many parameters. We need a way to deal with this situation without the mathematical and computational complexity of the full multivariate problem. \n",
    "\n",
    "Our approach is to factorize the joint distribution to introduce **global independence** and **local independence**. The global independence assumption allows us to factorize the **posterior distribution** over the conditional tables. The local independence assumption allows us to factor the prior distribution, a necessary condition for the global independence assumption. \n",
    "\n",
    "### Global independence assumption\n",
    "\n",
    "Let's start with the global independence assumption using an example. Recalling the student example of previous lessons, let's consider the parameterized joint distribution of the student's grade in the machine learning course. Using plate notation we call illustrate this part of the DAG as shown here.\n",
    "\n",
    "<img src=\"img/Factorizing.JPG\" alt=\"Drawing\" style=\"width:200px; height:300px\"/>\n",
    "<center> **Plate diagram for factorizing joint distribution $p(\\Theta_D, \\Theta_I, \\Theta_G)$** </center>\n",
    "\n",
    "Let's assume we can factorize the joint distribution as follows.\n",
    "\n",
    "$$p(\\Theta_D, \\Theta_I, \\Theta_G) = p(\\Theta_D)p(\\Theta_I)p(\\Theta_G)$$\n",
    "\n",
    "Were $\\Theta_D$, $\\Theta_I$, and $\\Theta_G$ are parameters of the distributions of difficulty, intelligence and grade respectively. \n",
    "\n",
    "If we assume the table (vector) of N data values, $\\mathcal{V}$, are i.i.d. we can expand the above as:\n",
    "\n",
    "$$p(\\Theta_D, \\Theta_I, \\Theta_G) = p(\\Theta_D)p(\\Theta_I)p(\\Theta_G) \\prod_{n=1}^N p(d^n\\ |\\ \\Theta_D)p(i^n\\ |\\ \\Theta_I)p(g^n\\ |\\ d^n, i^n, \\Theta_G)$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local parameter independence\n",
    "\n",
    "The local independence assumption allows us to factorize the prior distribution. Let's say that difficulty, $D$, and intelligence $I$, are only binomially distributed. We can factorize the prior distribution. For example: \n",
    "\n",
    "$$p(\\Theta_G) = p(\\Theta_G)^{0,0}p(\\Theta_G)^{0,1}p(\\Theta_G)^{1,0}p(\\Theta_G)^{1,1}$$\n",
    "\n",
    "Given this factorization we can find a factorization of the posterior distribution. \n",
    "\n",
    "$$p(\\Theta_G\\ |\\ \\mathcal{V}) \\propto p(\\mathcal{V}\\ |\\ \\Theta_G) p(\\Theta_G)^{0,0}p(\\Theta_G)^{0,1}p(\\Theta_G)^{1,0}p(\\Theta_G)^{1,1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to the Categorical and Dirichlet Distributions\n",
    "\n",
    "Many problems have multiple states or categories. The probabilities of the classes or categories are modeled with the **categorical distribution**. The categorical distribution is the multi-class generalization of the Bernoulli distribution. In fact, this distribution is sometime referred to as the **multinoulli distribution**. For Bayesian learning in these cases, the prior and posterior are modeled with the **Dirichlet distribution**. \n",
    "\n",
    "### The Categorical Distribution\n",
    "\n",
    "We can express the **probability mass** function of the categorical distribution, with $k$ categories, with probability $p_i$ for the ith category as follows.\n",
    "\n",
    "$$f(x = i\\ |\\ \\boldsymbol{p}) = p_i \\\\\n",
    "where\\\\\n",
    "\\boldsymbol{p} = (p_1, \\ldots, p_k),\\ and\\\\ \\sum_{i=1}^k p_i = 1.0$$\n",
    "\n",
    "An alternative formulation uses **Iverson bracket notation**. In this formulation $[x = i] = 1$ and $0$ otherwise. Using this notation the density function of the categorical distribution can be written:\n",
    "\n",
    "$$f(x = i\\ |\\ \\boldsymbol{p}) = \\prod_{i=1}^k p_i^{[x = i]}$$\n",
    "\n",
    "### The Dirichlet Distribution\n",
    "\n",
    "In order to perform Bayesian learning with the categorical distribution we need to use Dirichlet distribution as a prior. However, it can be a bit difficult to develop intuition around the behavior of the Dirichlet distribution. We can express the Dirichlet distribution with $n$ categories as follows.\n",
    "\n",
    "$$f(x_1, x_2, \\ldots, x_n; \\alpha_1, \\alpha_2, \\ldots, \\alpha_n) = \\frac{1}{B(\\alpha)} \\prod_{i=1}^{n} x_i^{\\alpha_i - 1}$$\n",
    "Where the normalization is the **Beta function**:\n",
    "$$B(\\alpha) = \\frac{\\prod_{i=1}^{n} \\Gamma (\\alpha_i) }{\\Gamma \\big( \\sum_{i=1}^{n} \\alpha_i \\big)}$$\n",
    "And $\\Gamma( x )$ is the Gamma function. \n",
    "\n",
    "For our case of interest, the categories are represented by $\\{ x_1, x_2, \\ldots, x_n \\}$ and are constrained by:\n",
    "$$\\sum_{i=1}^{n} x_i = 1\\\\ x_i \\ge 0$$\n",
    "\n",
    "In fact, in classification problems there is only one category per case, so $[x_i = 1]$ only for the ith category.  \n",
    "\n",
    "The **concentration parameters** $\\{ \\alpha_1, \\alpha_2, \\ldots, \\alpha_n \\}$, determines how *concentrated* the the distribution is for each class. For the Dirichlet distribution, **prior information** is expressed in terms of the concentration hyperparamters $\\boldsymbol{\\alpha} = \\{ \\alpha_1, \\alpha_2, \\ldots, \\alpha_n \\}$. The values of $\\alpha_i$ are parameterized in terms of **pseudocounts** for each category. The pseudocounts express prior information in two ways:\n",
    "- Categories with larger pseudocounts have likely prior probabilities. \n",
    "- The total sum of pseudocounts expresses the strength of the prior. If the sum of all pseudocounts is relatively large with respect to the number of actual data, the prior will be strong.\n",
    "- If all $\\alpha_i$ are equal the Dirichlet distribution is uniform in the probabilities of the classes.\n",
    "\n",
    "### Bayesian Parameter Estimation\n",
    "\n",
    "When computing the Bayesian MAP of $\\boldsymbol{p} = (p_1, \\ldots, p_k)$ the posterior is Dirichlet distributed. We will skip the considerable amount of algebra and just state that for a category, $i$, with $c_i$ counts out of $N$ total data values, $\\boldsymbol{X} = (x_1, x_2, \\ldots, x_n)$, the expected posterior MAP of $p_i$ can be expressed:\n",
    "\n",
    "$$E(p_i\\ |\\ \\boldsymbol{X}, \\boldsymbol{\\alpha}) = \\frac{c_i + \\alpha_i}{N + \\sum_k \\alpha_k}$$\n",
    "\n",
    "The above expression has an intuitive explanation. Consider the following:\n",
    "- The probability of a category $i$ increases as the count, $c_i$, increases relative to $N$, the sum of the counts for all categories. \n",
    "- As N increases relative to $\\sum_k \\alpha_k$ the data comes dominate the prior. On the other hand, if there are few actual data the prior will dominate. \n",
    "\n",
    "### Maximum Likelihood Estimation\n",
    "\n",
    "Computing the MLE for the categorical distribution is rather strait forward. The expression is the same as for MAP, but with all $\\alpha_i = 0$:\n",
    "\n",
    "$$E(p_i\\ |\\ \\boldsymbol{X}) = \\frac{c_i}{N}$$\n",
    "\n",
    "In other words, MLE probability for each category is just the faction of counts for that category in the dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computational Example Continued. \n",
    "\n",
    "We will now continue to simulate our dataset using the categorical distribution. Here, we will simulate the CPD of the student's grade, G. The basic steps are the same as before, but are a bit more complicated in this case since there are two parent variables and three possible output states. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_class(x):\n",
    "    \"\"\"\n",
    "    Function to flatten the array produced by the numpy.random.multinoulli function. \n",
    "    The function tests which binary value of the array of output states is true\n",
    "    and substitutes an integer for that state. This function only works for up to three\n",
    "    output states. \n",
    "    \n",
    "    Argument:\n",
    "    x - The array produced by the numpy.random.multinoulli function. \n",
    "    \"\"\"\n",
    "    out = []\n",
    "    for i,j in enumerate(x):\n",
    "        if j[0] == 1: out.append(0)\n",
    "        elif j[1] == 1: out.append(1)\n",
    "        else: out.append(2)   \n",
    "    return(out)   \n",
    "\n",
    "def sim_multinoulli(p, n = 25):\n",
    "    \"\"\"\n",
    "    Function to compute the vectors with probabilities for each \n",
    "    condition (input value) of the dependent variable using the multinoulli\n",
    "    distribution. \n",
    "    \n",
    "    The arguments are:\n",
    "    p - an array of probabilites of success for each possible combination\n",
    "        of states of the parent variables. Each row in the array are the \n",
    "        probabilities for each state of the multinoulli distribution for \n",
    "        that combination of parent values.\n",
    "    n - The numer of realizations. \n",
    "    \"\"\"\n",
    "    temp = np.zeros(shape = (p.shape[0] + 1, n))\n",
    "    for i in range(p.shape[1]): \n",
    "        ps = []\n",
    "        for j in range(p.shape[0]):\n",
    "            ps.append(p[j,i])   \n",
    "        mutlis = nr.multinomial(1, ps, n)    \n",
    "        temp[i,:] = set_class(mutlis)\n",
    "    return(temp)\n",
    "\n",
    "def selec_dist_2(sims, var1, var2, lg1, lg2):\n",
    "    \"\"\"\n",
    "    Function to integrate the conditional probabilities for\n",
    "    each of the cases of two parent variables. \n",
    "    \n",
    "    The arguments are:\n",
    "    sims - the array of simulated realizations with one row for each state of the\n",
    "           union of the parent variables. \n",
    "    var1 - the vector of values of first parent variable used to select the value from the \n",
    "          sims array.\n",
    "    var2 - the vector of values of second parent variable used to select the value from the \n",
    "          sims array.\n",
    "    lg1 - vector of states of possible states of the first parent variable. These must be\n",
    "         in the same order as for the sims array. \n",
    "    lg2 - vector of states of possible states of the second parent variable. These must be\n",
    "         in the same order as for the sims array. \n",
    "    \"\"\"\n",
    "    out = sims[0,:] # Copy values for first combination of states for parent variables\n",
    "    ## make sure the parent variables are 1-d numpy arrays.\n",
    "    var1 = np.array(var1).ravel() \n",
    "    var2 = np.array(var2).ravel()\n",
    "    for i in range(1, sims.shape[0]): # Loop over all comnination of states of the parent variables\n",
    "        out = [x if u == lg1[i] and v == lg2[i] else y for x,y,u,v in zip(sims[i,:], out, var1, var2)]\n",
    "    return([int(x) for x in out])\n",
    "\n",
    "# Specify the array of probabilities for the states. \n",
    "probs = np.array([[0.3, 0.05, 0.8,  0.5], [0.4, 0.25, 0.15, 0.3], [0.3, 0.7,  0.05, 0.2]])\n",
    "nr.seed(2343)\n",
    "sims = sim_multinoulli(probs, nsamp)\n",
    "\n",
    "## Specify the possible combinations of states of the parent variables. \n",
    "I_lg = [0,0,1,1]\n",
    "D_lg = [0,1,0,1]\n",
    "\n",
    "## Simualte the CPD\n",
    "G_samps = pd.DataFrame(selec_dist_2(sims, I_samps, D_samps, I_lg, D_lg), columns = ['G'])\n",
    "G_samps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the CPD computed we can estimate the parameters. By using a DAG with only this one variable, the parameters estimated will be for a marginal distribution which makes interpretation a bit difficult. Still, it is worth trying the test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = BayesianModel()\n",
    "test_model.add_node('G')\n",
    "\n",
    "# Fitting the data to the model using Maximum Likelihood Estimator\n",
    "test_model.fit(G_samps, estimator=MaximumLikelihoodEstimator)\n",
    "print(test_model.get_cpds('G'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameter estimates seem reasonable, if a bit hard to understand. \n",
    "\n",
    "There is one last variable to simulate, the letter, L. This CPD has the parent G, with three states, requiring the specification of three success probabilities.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = [0.9, 0.6, 0.1]\n",
    "bern = sim_bernoulli(probs, nsamp)\n",
    "\n",
    "nr.seed(2234)\n",
    "L_samps = pd.DataFrame(selec_dist_1(bern, G_samps, [0,1,2]), columns = ['L'])\n",
    "L_samps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will estimate the parameters of the (marginal) distribution as an approximate test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = BayesianModel()\n",
    "test_model.add_node('L')\n",
    "\n",
    "# Fitting the data to the model using Maximum Likelihood Estimator\n",
    "test_model.fit(L_samps, estimator=MaximumLikelihoodEstimator)\n",
    "print(test_model.get_cpds('L'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimation of Model Parameters\n",
    "\n",
    "With the simulation created we can now estimate the parameters of the CPDs in our model. First, we will use the maximum likelihood method. Then, we will apply a Bayes method. \n",
    "\n",
    "## MLE\n",
    "\n",
    "As a first step in estimating the parameters, a data frame must be constructed from the variables.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Bind the variables into one data frame\n",
    "dats = pd.concat([I_samps, D_samps, S_samps, G_samps, L_samps], axis = 1)\n",
    "dats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the data frame constructed, we can estimate the parameters of our model. There are a total of 26 model parameters which must be fit using the 25 data points. Not only, but the distribution of the cases are not uniform given the sampling. This model is seriously **under-determined**. This situation will expose one of the significant weaknesses of MLE.    \n",
    "\n",
    "The code in the cell below performs the following steps for the MLE of the model parameters:\n",
    "1. The structure of the DAG was defined.\n",
    "2. The parameters of the model are estimated using the `MaximumLikelihoodEstimator` in the `fit` method. \n",
    "3. The parameters of the CDPs are printed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pgmpy.models import BayesianModel\n",
    "from pgmpy.estimators import MaximumLikelihoodEstimator, BayesianEstimator\n",
    "\n",
    "student_model = BayesianModel([('D', 'G'), ('I', 'G'), ('G', 'L'), ('I', 'S')])\n",
    "\n",
    "# Learing CPDs using Maximum Likelihood Estimators\n",
    "student_model.fit(dats, estimator=MaximumLikelihoodEstimator)\n",
    "for cpd in student_model.get_cpds():\n",
    "    print(\"CPD of {variable}:\".format(variable=cpd.variable))\n",
    "    print(cpd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The estimates of the conditionally independent variables are as expected. Some of the other CDPs look a bit odd. Notice the 0s in the G variable. The L and S variables are uniformly distributed, which is unexpected. It is possible that much of this behavior is attributable to the under-determined nature of the problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian Estimation\n",
    "\n",
    "Next, we will turn our attention to Bayesian parameter estimation. Recall that we performed Bayesian MAP estimation on the variable D using pseudo counts of $\\{ 10,10 \\}$, a uniform and modestly strong prior. This gave parameter values of $\\{ 0.45, 0.55 \\}$.\n",
    "\n",
    "Let's explore the effect of using different priors. In the cell below, the parameters of the variable D are estimated using a strong prior, with pseudo counts $\\{ 50, 50 \\}$. This is considered a strong prior since the pseudo counts are larger than the total number of data cases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = BayesianModel()\n",
    "test_model.add_node('D')\n",
    "\n",
    "# Fitting the data to the model using Bayesian Estimator\n",
    "pseudo_counts = {'D': [50, 50]}\n",
    "test_model.fit(D_samps, estimator=BayesianEstimator, prior_type='dirichlet', pseudo_counts=pseudo_counts)\n",
    "print(test_model.get_cpds('D'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the strong prior, the parameter estimates are close to the proportion of the pseudo counts, that is close to the prior. \n",
    "\n",
    "We can also add a **bias** to the parameter estimates through specification of the prior. In the code below the pseudo counts are $\\{ 5, 10 \\}$. This relatively weak prior favors the positive case over the negative case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = BayesianModel()\n",
    "test_model.add_node('D')\n",
    "\n",
    "# Fitting the data to the model using Bayesian Estimator\n",
    "pseudo_counts = {'D': [4, 8]}\n",
    "test_model.fit(D_samps, estimator=BayesianEstimator, prior_type='dirichlet', pseudo_counts=pseudo_counts)\n",
    "print(test_model.get_cpds('D'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These parameters are close to the prior values, despite the relatively weak prior. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_model = BayesianModel([('D', 'G'), ('I', 'G'), ('G', 'L'), ('I', 'S')])\n",
    "\n",
    "# Learing CPDs using Bayesian Estimators\n",
    "pseudo_counts = {'D': [4, 6], 'I':[4,6], 'G':[3,3,3], 'S':[4,6], 'L':[4,6]}\n",
    "student_model.fit(dats, estimator=BayesianEstimator, prior_type='dirichlet', pseudo_counts=pseudo_counts)\n",
    "for cpd in student_model.get_cpds():\n",
    "    print(\"CPD of {variable}:\".format(variable=cpd.variable))\n",
    "    print(cpd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the following:\n",
    "- The parameters of the conditionally independent variables, D and I, are nearly the same as obtained with the MLE method.  \n",
    "- The parameter estimates for the variable G now seem more as we might expect. \n",
    "- The parameter estimates for the variables L and S appear to reflect the prior values of the parameters. \n",
    "\n",
    "The stabilization of the parameter estimates for the variable G is an example of **Bayesian regularization**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLE with more data\n",
    "\n",
    "You may wonder if simply acquiring more data might help the MLE method behave better. In other words, is the small number of cases the only reason for poor MLE performance. Let's explore this aspect of the problem by simulating 10 times as much data. The code in the cell below simulates 250 cases using the same probabilities as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's try a larger sample\n",
    "nsamp = 250\n",
    "\n",
    "## First the conditionally independent variables\n",
    "nr.seed(22234)\n",
    "D_samps = pd.DataFrame(nr.binomial(1, 0.7, nsamp), columns = ['D'])\n",
    "nr.seed(2355)\n",
    "I_samps = pd.DataFrame(nr.binomial(1, 0.8, nsamp), columns = ['I'])\n",
    "\n",
    "## The variable conditional on two others and with three possible states\n",
    "probs = np.array([[0.3, 0.05, 0.8,  0.5], [0.4, 0.25, 0.15, 0.3], [0.3, 0.7,  0.05, 0.2]])\n",
    "nr.seed(2334)\n",
    "sims = sim_multinoulli(probs, nsamp)\n",
    "I_lg = [0,0,1,1]\n",
    "D_lg = [0,1,0,1]\n",
    "G_samps = pd.DataFrame(selec_dist_2(sims, I_samps, D_samps, I_lg, D_lg), columns = ['G'])\n",
    "\n",
    "## Finally, the two variables conditionally depenent on one other\n",
    "probs = [0.9, 0.6, 0.1]\n",
    "nr.seed(2134)\n",
    "bern = sim_bernoulli(probs, nsamp)\n",
    "L_samps = pd.DataFrame(selec_dist_1(bern, G_samps, [0,1,2]), columns = ['L'])\n",
    "\n",
    "probs = [0.2, 0.8]\n",
    "nr.seed(22234)\n",
    "bern = sim_bernoulli(probs)\n",
    "S_samps = pd.DataFrame(selec_dist_1(bern, I_samps, [0,1]), columns = ['S'])\n",
    "\n",
    "## Now concatenate the columns into one data frame\n",
    "dats = pd.concat([I_samps, D_samps, S_samps, G_samps, L_samps], axis = 1)\n",
    "print(dats.shape)\n",
    "dats.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will compute the parameters for a model using the 250 data cases using MLE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_model = BayesianModel([('D', 'G'), ('I', 'G'), ('G', 'L'), ('I', 'S')])\n",
    "\n",
    "# Learing CPDs using Maximum Likelihood Estimators\n",
    "student_model.fit(dats, estimator=MaximumLikelihoodEstimator)\n",
    "for cpd in student_model.get_cpds():\n",
    "    print(\"CPD of {variable}:\".format(variable=cpd.variable))\n",
    "    print(cpd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results are not much different from the ones obtained with 25 cases in the dataset. Evidently, lack of data was not the key problem. Rather, the structure of the model could be the limitation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Copyright 2018 Stephen F Elston. All rights reserved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
