{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Dynamic Programming\n",
    "\n",
    "## CSCI E-82A\n",
    "## Stephen Elston\n",
    "\n",
    "In the previous lesson we explored the concepts of **Markov decision processes (MDP)** and **Markov reward processes (MRP)**. Now we will extend these concepts and apply them to finding **optimal solutions** for such systems. By an optimal solution, we mean a solution which produces **greater reward** than any other solution. \n",
    "\n",
    "To understand how we can find optimal solutions we must introduce some new concepts:\n",
    "1. An **action** causes a state transition. This transition may be to the same state. \n",
    "2. A **policy** specifies the **actions** given the state. In other words, the policy defines the actions to be specified by the agent. \n",
    "3. An **optimal policy** produces the greatest reward possible given the initial state of the system.\n",
    "4. A **plan** is the sequence of actions leading to an optimal result. \n",
    "\n",
    "In this an subsequent lessons, we will explore some powerful methods for finding optimal policies. Broadly, these methods are known as **dynamic programming** and **reinforcement learning**. In this lesson we will focus on the representation and learning methods for dynamic programming. Dynamic programming algorithms can be the basis of effective and flexible intelligent agents as shown below.\n",
    "\n",
    "<img src=\"img/DPAgent.JPG\" alt=\"Drawing\" style=\"width:500px; height:300px\"/>\n",
    "<center> **Dynamic Programming Agent and Environment** </center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Functions and Policy Evaluation\n",
    "\n",
    "Given a Markov random process, a reward function, and a policy of actions on the Markov process, how can we tell how good our policy is? We can perform **policy evaluation** in two ways.  \n",
    "\n",
    "First, we can perform **policy evaluation** with a **value function**. The value function is the expected value of the **gain** achieved by following a policy, $\\pi$, given the current state. We can express the state value function as follows:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "v_{\\pi}(s) &= \\mathbb{E}_{\\pi} [ G_t\\ |\\ S_t = s] \\\\\n",
    "&= \\mathbb{E}_{\\pi} [R_{t+1} + \\gamma G_{t+1}\\ |\\ S_t = s] \\\\\n",
    "&= \\mathbb{E}_{\\pi} [R_{t+1} + \\gamma v_{\\pi} (S_{t+1})\\ |\\ S_t = s] \\\\\n",
    "&= \\sum_a \\pi(a|s) \\sum_{s',r} p(s',r | s,a) \\big[ r + \\gamma v_{\\pi}(s') \\big],\\ \\forall a \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "This relation are known as the **Bellman value equations**. This relationship tells us how to compute the value of being in a particular state, $s$. There is one such equation for each state, $s \\in \\xi$, of the Markov process.\n",
    "\n",
    "Examine the last line of the above and notice that this relationship can be viewed as a recursion.    \n",
    "\n",
    "<img src=\"img/ValueBackup.JPG\" alt=\"Drawing\" style=\"width:300px; height:200px\"/>\n",
    "<center> **Backups diagram of Bellman Value Function** </center>\n",
    "\n",
    "As an alternative we can use the **Bellman action value function**:\n",
    "\n",
    "$$\\begin{align}\n",
    "q_{*}(s,a) &= \\mathbb{E_\\pi} \\big[G_{t}\\ \\big|\\ S_t = s, A_t = a \\big]  \\\\\n",
    "&= \\mathbb{E_\\pi} \\Big[ \\sum_{k=0}^\\infty \\gamma^k\\ R_{t+k+1} \\big|\\ S_t = s, A_t = a \\Big]  \\\\\n",
    "&= \\sum_{s',r} p(s',r\\ |\\ s,a) \\big[ r + \\gamma\\ q_{*}(S_{t+1},a') \\big]\n",
    "\\end{align}$$\n",
    "\n",
    "Whereas, $v_{\\pi}(s)$ is the value of being in a state, $q_{*}(s,a)$ tells us the value of taking a particular action, $a$, from a state, $s$. There is one such equation for each action value tuple, $(a,s)$.\n",
    "\n",
    "<img src=\"img/ActionValueBackup.JPG\" alt=\"Drawing\" style=\"width:200px; height:200px\"/>\n",
    "<center> **Backups diagram of Bellman Action Value Function** </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid World Example\n",
    "\n",
    "Let's try an example of computing the state values of a Markov process. In this example, we will work with both the **representations** and **learning** required. \n",
    "\n",
    "**Navigation** to a goal is a significant problem in robotics. Real-world navigation is rather complex. Therefore, in this example we will use a simple analog called a **grid world**. The grid world for this problem is shown below. \n",
    "\n",
    "<img src=\"img/GridWorld.JPG\" alt=\"Drawing\" style=\"width:200px; height:200px\"/>\n",
    "<center> **A 4x4 Grid World with Terminal State** </center>\n",
    "\n",
    "The grid world consists of a 4x4 set of positions the robot can occupy. each position is considered a state. The goal is to navigate to state 0, the goal, in the minimum steps. We will explore methods to find policies which reach this goal and achieve maximum reward. \n",
    "\n",
    "Grid position 0 is a **terminal node**. There are no possible state transitions out of this position. The presence of a terminal node makes this an **episodic Markov random process**. In each episode the robot can start in any other random position, $\\{ 1,2,3,4,5,6,7,8,9,10,11,12,13,14,15 \\}$, and the episode terminates when the robot enters position (state) 0.  \n",
    "\n",
    "In each state, there are four possible actions the robot can take:\n",
    "- up, u\n",
    "- down, d,\n",
    "- left, l\n",
    "- right, r\n",
    "\n",
    "We encode, or represent, these possibilities in a dictionary as shown in the code block below. We use a dictionary of dictionaries to perform the lookup. The keys of the outer dictionary are the identifiers (numbers) of the states. The keys of the inner dictionary are the possible actions and the values are the **successor state**, $s'$, for that transition.  \n",
    "\n",
    "Notice that there are no allowed transitions out of the terminal state. Also, any transition that takes the robot off the grid, leaves the state unchanged. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import numpy for latter\n",
    "import numpy as np\n",
    "\n",
    "## Define the transition dictonary of dictionaries:\n",
    "policy = {0:{'u':0, 'd':0, 'l':0, 'r':0},\n",
    "          1:{'u':1, 'd':5, 'l':0, 'r':2},\n",
    "          2:{'u':2, 'd':6, 'l':1, 'r':3},\n",
    "          3:{'u':3, 'd':7, 'l':2, 'r':3},\n",
    "          4:{'u':0, 'd':8, 'l':4, 'r':5},\n",
    "          5:{'u':1, 'd':9, 'l':4, 'r':6},\n",
    "          6:{'u':2, 'd':10, 'l':5, 'r':7},\n",
    "          7:{'u':3, 'd':11, 'l':6, 'r':7},\n",
    "          8:{'u':4, 'd':12, 'l':8, 'r':9},\n",
    "          9:{'u':5, 'd':13, 'l':8, 'r':10},\n",
    "          10:{'u':6, 'd':14, 'l':9, 'r':11},\n",
    "          11:{'u':7, 'd':15, 'l':10, 'r':11},\n",
    "          12:{'u':8, 'd':12, 'l':12, 'r':13},\n",
    "          13:{'u':9, 'd':13, 'l':12, 'r':14},\n",
    "          14:{'u':10, 'd':14, 'l':13, 'r':15},\n",
    "          15:{'u':11, 'd':15, 'l':14, 'r':15},}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to define the initial transition probabilities for the Markov process. Initially, we set the probabilities for each transition as **uniform** or random. As there are 4 possible transitions from each state, this means all transition probabilities are 0.25. In other words, this is a random policy which does not favor any particular transitions. \n",
    "\n",
    "The initial uniform transition probabilities are encoded using a dictionary of dictionaries. The organization of this data structure is identical to the foregoing data structure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_to_state_probs = {0:{'u':0.0, 'd':0.0, 'l':0.0, 'r':0.0},\n",
    "                        1:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25}, \n",
    "                        2:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                        3:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                        4:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                        5:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                        6:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                        7:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                        8:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                        9:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                        10:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                        11:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                        12:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                        13:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                        14:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                        15:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25}}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The robot receives the following rewards:\n",
    "- 10 for entering position 0, \n",
    "- -1 for attempting to leave the grid. In other words, we are , \n",
    "- -0.1 for all other state transitions.\n",
    "\n",
    "We encode this reward in the same type of dictionary structure used for the foregoing structures.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = {0:{'u':0.0, 'd':0.0, 'l':0.0, 'r':0.0},\n",
    "          1:{'u':-1, 'd':-0.1, 'l':10.0, 'r':-0.1},\n",
    "          2:{'u':-1.0, 'd':-0.1, 'l':-0.1, 'r':-0.1},\n",
    "          3:{'u':-1.0, 'd':-0.1, 'l':-0.1, 'r':-1.0},\n",
    "          4:{'u':10.0, 'd':-0.1, 'l':-1.0, 'r':-0.1},\n",
    "          5:{'u':-0.1, 'd':-0.1, 'l':-0.1, 'r':-0.1},\n",
    "          6:{'u':-0.1, 'd':-0.1, 'l':-0.1, 'r':-0.1},\n",
    "          7:{'u':-0.1, 'd':-0.1, 'l':-0.1, 'r':-1.0},\n",
    "          8:{'u':-0.1, 'd':-0.1, 'l':-1.0, 'r':-0.1},\n",
    "          9:{'u':-0.1, 'd':-0.1, 'l':-0.1, 'r':-0.1},\n",
    "          10:{'u':-0.1, 'd':-0.1, 'l':-0.1, 'r':-0.1},\n",
    "          11:{'u':-0.1, 'd':-0.1, 'l':-0.1, 'r':-1.0},\n",
    "          12:{'u':-0.1, 'd':-1.0, 'l':-1.0, 'r':-0.1},\n",
    "          13:{'u':-0.1, 'd':-1.0, 'l':-0.1, 'r':-0.1},\n",
    "          14:{'u':-0.1, 'd':-1.0, 'l':-0.1, 'r':-0.1},\n",
    "          15:{'u':-0.1, 'd':-1.0, 'l':-0.1, 'r':-1.0}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we have constructed a rather poor policy. This policy is just a random walk around the grid. Still, we can still measure the value of this policy. \n",
    "\n",
    "The function in the code below iterates over the Bellman value function to find the values of each state. The iteration continues until the convergence criteria is meet.  \n",
    "\n",
    "> **Note:** The code in this example takes advantage of the fact that there is only one possible successor state for each action. This means there is no need to sum over successor states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.92461824, -3.93184927, -6.48887002],\n",
       "       [ 0.92461824, -2.10995108, -4.95119803, -6.86735717],\n",
       "       [-3.93184927, -4.95119803, -6.5102566 , -7.87700873],\n",
       "       [-6.48887002, -6.86735717, -7.87700873, -8.96905736]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_state_value(pi, probs, reward, gamma = 1.0, theta = 0.01, display = False):\n",
    "    '''Function for policy evaluation  \n",
    "    '''\n",
    "    delta = theta\n",
    "    values = np.zeros(len(probs)) # Initialize the value array\n",
    "    while(delta >= theta):\n",
    "        v = np.copy(values) ## save the values for computing the difference later\n",
    "        for s in probs.keys():\n",
    "            temp_values = 0.0 ## Initial the sum of values for this state\n",
    "            for action in rewards[s].keys():\n",
    "                s_prime = pi[s][action]\n",
    "                temp_values = temp_values + probs[s][action] * (reward[s][action] + gamma * values[s_prime])\n",
    "            values[s] = temp_values\n",
    "            \n",
    "        ## Compute the differences to see convergence has been reached.    \n",
    "        diffs = np.sum(np.abs(np.subtract(v, values)))\n",
    "        delta = min([delta, diffs])\n",
    "        if(display): \n",
    "            print('difference metric = ' + str(diffs))\n",
    "            print(values.reshape(4,4))\n",
    "    return values\n",
    "\n",
    "compute_state_value(policy, state_to_state_probs, rewards, theta = 0.1, display = False).reshape(4,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of Dynamic Programming\n",
    "\n",
    "What is **dynamic programming**? In the most general terms, dynamic programming is a **planning method**. A planning method is a means for an intelligent agent to gain improved autonomy though a sequence of actions to achieve a **goal**. \n",
    "\n",
    "Dynamic programming was developed in the 1950's by mathematician **Richard Bellman**. By *programming* Bellman meant a computer algorithm which optimizes the **value** of the states visited in a system represented by a Markov process. By *dynamic*, Bellman meant the algorithm solves the problem recursively by operating on smaller and simpler sub-problems. \n",
    "\n",
    "The key idea of dynamic programming is expressed by the **Bellman optimality equations**. There are two ways we can express this optimal relationship in two different ways. First, we can find a relationship which is a solution to the **optimal value function**, $v_{*}(s)$: \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "v_{*}(s) &= max_a\\ \\mathbb{E} [ G_{t+1} + \\gamma v_*(S_{t+1})\\ |\\ S_t = s, A_t = a] \\\\\n",
    "&= max_a\\ \\sum_{s',r} p(s',r\\ |\\ s,a) \\big[ r + \\gamma v_{*}(s') \\big]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The other possibility is to use the other form of the Bellman optimality equations. This is the **optimal state action** relationship:\n",
    "\n",
    "$$\\begin{align}\n",
    "q_{*}(s,a) &= \\mathbb{E} \\big[R_{t+1} + \\gamma max_{a'}\\ q_{*}(S_{t+1},a')\\ \n",
    "\\big|\\ S_t = s, A_t = a \\big]  \\\\\n",
    "&= max_a \\sum_{s',r} p(s',r\\ |\\ s,a) \\big[ r + \\gamma\\ max_{a'}\\ q_{*}(S_{t+1},a') \\big]\n",
    "\\end{align}$$\n",
    "\n",
    "Like dynamic programming, **reinforcement learning** is class of optimization algorithms using a sequence of actions for a system represented by a Markov processes.Therefore, understanding dynamic programming is good path to understanding reinforcement learning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Iteration for Grid World"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "diff = 128.1086520299247\n",
      "Current policy\n",
      "{0: {'u': 0.25, 'd': 0.25, 'l': 0.25, 'r': 0.25}, 1: {'u': 0.0, 'd': 0.0, 'l': 1.0, 'r': 0.0}, 2: {'u': 0.0, 'd': 0.3333333333333333, 'l': 0.3333333333333333, 'r': 0.3333333333333333}, 3: {'u': 0.0, 'd': 0.5, 'l': 0.5, 'r': 0.0}, 4: {'u': 1.0, 'd': 0.0, 'l': 0.0, 'r': 0.0}, 5: {'u': 0.25, 'd': 0.25, 'l': 0.25, 'r': 0.25}, 6: {'u': 0.25, 'd': 0.25, 'l': 0.25, 'r': 0.25}, 7: {'u': 0.3333333333333333, 'd': 0.3333333333333333, 'l': 0.3333333333333333, 'r': 0.0}, 8: {'u': 0.3333333333333333, 'd': 0.3333333333333333, 'l': 0.0, 'r': 0.3333333333333333}, 9: {'u': 0.25, 'd': 0.25, 'l': 0.25, 'r': 0.25}, 10: {'u': 0.25, 'd': 0.25, 'l': 0.25, 'r': 0.25}, 11: {'u': 0.3333333333333333, 'd': 0.3333333333333333, 'l': 0.3333333333333333, 'r': 0.0}, 12: {'u': 0.5, 'd': 0.0, 'l': 0.0, 'r': 0.5}, 13: {'u': 0.3333333333333333, 'd': 0.0, 'l': 0.3333333333333333, 'r': 0.3333333333333333}, 14: {'u': 0.3333333333333333, 'd': 0.0, 'l': 0.3333333333333333, 'r': 0.3333333333333333}, 15: {'u': 0.5, 'd': 0.0, 'l': 0.5, 'r': 0.0}}\n",
      "With state values\n",
      "[[ 0.         10.          8.82261553  8.36513351]\n",
      " [10.          9.10497156  8.41694102  8.11572316]\n",
      " [ 8.82261553  8.41694102  8.04101798  7.87394536]\n",
      " [ 8.36513351  8.11572316  7.87394536  7.77394536]]\n",
      "\n",
      "diff = 18.591347970075287\n",
      "Current policy\n",
      "{0: {'u': 0.25, 'd': 0.25, 'l': 0.25, 'r': 0.25}, 1: {'u': 0.0, 'd': 0.0, 'l': 1.0, 'r': 0.0}, 2: {'u': 0.0, 'd': 0.0, 'l': 1.0, 'r': 0.0}, 3: {'u': 0.0, 'd': 0.0, 'l': 1.0, 'r': 0.0}, 4: {'u': 1.0, 'd': 0.0, 'l': 0.0, 'r': 0.0}, 5: {'u': 0.5, 'd': 0.0, 'l': 0.5, 'r': 0.0}, 6: {'u': 0.0, 'd': 0.0, 'l': 1.0, 'r': 0.0}, 7: {'u': 0.0, 'd': 0.0, 'l': 1.0, 'r': 0.0}, 8: {'u': 1.0, 'd': 0.0, 'l': 0.0, 'r': 0.0}, 9: {'u': 1.0, 'd': 0.0, 'l': 0.0, 'r': 0.0}, 10: {'u': 0.5, 'd': 0.0, 'l': 0.5, 'r': 0.0}, 11: {'u': 1.0, 'd': 0.0, 'l': 0.0, 'r': 0.0}, 12: {'u': 1.0, 'd': 0.0, 'l': 0.0, 'r': 0.0}, 13: {'u': 1.0, 'd': 0.0, 'l': 0.0, 'r': 0.0}, 14: {'u': 0.0, 'd': 0.0, 'l': 1.0, 'r': 0.0}, 15: {'u': 0.5, 'd': 0.0, 'l': 0.5, 'r': 0.0}}\n",
      "With state values\n",
      "[[ 0.  10.   9.9  9.8]\n",
      " [10.   9.9  9.8  9.7]\n",
      " [ 9.9  9.8  9.7  9.6]\n",
      " [ 9.8  9.7  9.6  9.5]]\n",
      "\n",
      "diff = 0.0\n",
      "Current policy\n",
      "{0: {'u': 0.25, 'd': 0.25, 'l': 0.25, 'r': 0.25}, 1: {'u': 0.0, 'd': 0.0, 'l': 1.0, 'r': 0.0}, 2: {'u': 0.0, 'd': 0.0, 'l': 1.0, 'r': 0.0}, 3: {'u': 0.0, 'd': 0.0, 'l': 1.0, 'r': 0.0}, 4: {'u': 1.0, 'd': 0.0, 'l': 0.0, 'r': 0.0}, 5: {'u': 0.5, 'd': 0.0, 'l': 0.5, 'r': 0.0}, 6: {'u': 0.0, 'd': 0.0, 'l': 1.0, 'r': 0.0}, 7: {'u': 0.0, 'd': 0.0, 'l': 1.0, 'r': 0.0}, 8: {'u': 1.0, 'd': 0.0, 'l': 0.0, 'r': 0.0}, 9: {'u': 1.0, 'd': 0.0, 'l': 0.0, 'r': 0.0}, 10: {'u': 0.5, 'd': 0.0, 'l': 0.5, 'r': 0.0}, 11: {'u': 1.0, 'd': 0.0, 'l': 0.0, 'r': 0.0}, 12: {'u': 1.0, 'd': 0.0, 'l': 0.0, 'r': 0.0}, 13: {'u': 1.0, 'd': 0.0, 'l': 0.0, 'r': 0.0}, 14: {'u': 0.0, 'd': 0.0, 'l': 1.0, 'r': 0.0}, 15: {'u': 0.5, 'd': 0.0, 'l': 0.5, 'r': 0.0}}\n",
      "With state values\n",
      "[[ 0.  10.   9.9  9.8]\n",
      " [10.   9.9  9.8  9.7]\n",
      " [ 9.9  9.8  9.7  9.6]\n",
      " [ 9.8  9.7  9.6  9.5]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: {'d': 0.25, 'l': 0.25, 'r': 0.25, 'u': 0.25},\n",
       " 1: {'d': 0.0, 'l': 1.0, 'r': 0.0, 'u': 0.0},\n",
       " 2: {'d': 0.0, 'l': 1.0, 'r': 0.0, 'u': 0.0},\n",
       " 3: {'d': 0.0, 'l': 1.0, 'r': 0.0, 'u': 0.0},\n",
       " 4: {'d': 0.0, 'l': 0.0, 'r': 0.0, 'u': 1.0},\n",
       " 5: {'d': 0.0, 'l': 0.5, 'r': 0.0, 'u': 0.5},\n",
       " 6: {'d': 0.0, 'l': 1.0, 'r': 0.0, 'u': 0.0},\n",
       " 7: {'d': 0.0, 'l': 1.0, 'r': 0.0, 'u': 0.0},\n",
       " 8: {'d': 0.0, 'l': 0.0, 'r': 0.0, 'u': 1.0},\n",
       " 9: {'d': 0.0, 'l': 0.0, 'r': 0.0, 'u': 1.0},\n",
       " 10: {'d': 0.0, 'l': 0.5, 'r': 0.0, 'u': 0.5},\n",
       " 11: {'d': 0.0, 'l': 0.0, 'r': 0.0, 'u': 1.0},\n",
       " 12: {'d': 0.0, 'l': 0.0, 'r': 0.0, 'u': 1.0},\n",
       " 13: {'d': 0.0, 'l': 0.0, 'r': 0.0, 'u': 1.0},\n",
       " 14: {'d': 0.0, 'l': 1.0, 'r': 0.0, 'u': 0.0},\n",
       " 15: {'d': 0.0, 'l': 0.5, 'r': 0.0, 'u': 0.5}}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import copy\n",
    "def policy_iteration(pi, probs, reward, gamma = 1.0, theta = 0.1, output = False):\n",
    "    delta = theta\n",
    "    v = np.zeros(len(probs))\n",
    "    state_values = np.zeros(len(probs))\n",
    "    current_policy = copy.deepcopy(probs)\n",
    "    while(delta >= theta):\n",
    "        for s in probs.keys():\n",
    "            temp_values = []\n",
    "            for action in rewards[s].keys():\n",
    "                s_prime = pi[s][action]\n",
    "                temp_values.append(current_policy[s][action] * (reward[s][action] + gamma * state_values[s_prime]))\n",
    "            \n",
    "            ## Find the max value and update current policy\n",
    "            max_index = np.where(np.array(temp_values) == max(temp_values))[0]\n",
    "            prob_for_policy = 1.0/float(len(max_index))\n",
    "            for i,action in enumerate(current_policy[s].keys()): \n",
    "                if(i in max_index): current_policy[s][action] = prob_for_policy\n",
    "                else: current_policy[s][action] = 0.0\n",
    "                \n",
    "        \n",
    "        ## Compute state values with new policy to determine if there is an improvement\n",
    "        state_values = compute_state_value(pi, current_policy, rewards, theta = .1)\n",
    "        diff = np.sum(np.abs(np.subtract(v, state_values)))\n",
    "        if(output): \n",
    "            print('\\ndiff = ' + str(diff))\n",
    "            print('Current policy')\n",
    "            print(current_policy)\n",
    "            print('With state values')\n",
    "            print(state_values.reshape(4,4))\n",
    "        \n",
    "        delta = min([delta, np.sum(np.abs(np.subtract(v, state_values)))])\n",
    "        v = np.copy(state_values) \n",
    "    return current_policy\n",
    "\n",
    "policy_iteration(policy, state_to_state_probs, rewards, gamma = 1.0, output = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Iteration for Grid World"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference = 146.70000000000002\n",
      "[[ 0.  10.   9.9  9.8]\n",
      " [10.   9.9  9.8  9.7]\n",
      " [ 9.9  9.8  9.7  9.6]\n",
      " [ 9.8  9.7  9.6  9.5]]\n",
      "Difference = 0.0\n",
      "[[ 0.  10.   9.9  9.8]\n",
      " [10.   9.9  9.8  9.7]\n",
      " [ 9.9  9.8  9.7  9.6]\n",
      " [ 9.8  9.7  9.6  9.5]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: {'d': 0.25, 'l': 0.25, 'r': 0.25, 'u': 0.25},\n",
       " 1: {'d': 0.0, 'l': 0.0, 'r': 0.0, 'u': 1.0},\n",
       " 2: {'d': 0.0, 'l': 1.0, 'r': 0.0, 'u': 0.0},\n",
       " 3: {'d': 0.0, 'l': 1.0, 'r': 0.0, 'u': 0.0},\n",
       " 4: {'d': 0.0, 'l': 1.0, 'r': 0.0, 'u': 0.0},\n",
       " 5: {'d': 0.0, 'l': 0.5, 'r': 0.0, 'u': 0.5},\n",
       " 6: {'d': 0.0, 'l': 0.5, 'r': 0.0, 'u': 0.5},\n",
       " 7: {'d': 0.0, 'l': 0.5, 'r': 0.0, 'u': 0.5},\n",
       " 8: {'d': 0.0, 'l': 0.0, 'r': 0.0, 'u': 1.0},\n",
       " 9: {'d': 0.0, 'l': 0.5, 'r': 0.0, 'u': 0.5},\n",
       " 10: {'d': 0.0, 'l': 0.5, 'r': 0.0, 'u': 0.5},\n",
       " 11: {'d': 0.0, 'l': 0.5, 'r': 0.0, 'u': 0.5},\n",
       " 12: {'d': 0.0, 'l': 0.0, 'r': 0.0, 'u': 1.0},\n",
       " 13: {'d': 0.0, 'l': 0.5, 'r': 0.0, 'u': 0.5},\n",
       " 14: {'d': 0.0, 'l': 0.5, 'r': 0.0, 'u': 0.5},\n",
       " 15: {'d': 0.0, 'l': 0.5, 'r': 0.0, 'u': 0.5}}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def value_iteration(pi, probs, reward, gamma = 1.0, theta = 0.1, output = False):\n",
    "    delta = theta\n",
    "    v = np.zeros(len(probs))\n",
    "    state_values = np.zeros(len(probs))\n",
    "    current_policy = copy.deepcopy(probs)\n",
    "    while(delta >= theta):\n",
    "        for s in probs.keys(): # iteratve over all states\n",
    "            temp_values = []\n",
    "            ## Find the values for all possible actions in the state.\n",
    "            for action in rewards[s].keys():\n",
    "                s_prime = pi[s][action]\n",
    "                temp_values.append((reward[s][action] + gamma * state_values[s_prime]))\n",
    "            \n",
    "            ## Find the max value and update the value for the state\n",
    "            state_values[s] = max(temp_values)\n",
    "        ## Determine if convergence is achieved\n",
    "        diff = np.sum(np.abs(np.subtract(v, state_values)))\n",
    "        delta = min([delta, np.sum(np.abs(np.subtract(v, state_values)))])\n",
    "        v = np.copy(state_values)\n",
    "        if(output):\n",
    "            print('Difference = ' + str(diff))\n",
    "            print(state_values.reshape(4,4))\n",
    "    \n",
    "    ## Now we need to find the policy that makes max value state transitions\n",
    "    for s in current_policy.keys(): # iterate over all states\n",
    "        ## Find the indicies of maximum state transition values\n",
    "        temp_values = [state_values[pi[s][s_prime]] for s_prime in pi[s].keys()]\n",
    "        max_index = np.where(np.array(temp_values) == max(temp_values))[0]    \n",
    "        prob_for_policy = 1.0/float(len(max_index)) ## Probabilities of transition\n",
    "        for i, key in enumerate(current_policy[s]): ## Update policy\n",
    "            if(i in max_index): current_policy[s][key] = prob_for_policy\n",
    "            else: current_policy[s][key] = 0.0    \n",
    "    return current_policy\n",
    "\n",
    "value_iteration(policy, state_to_state_probs, rewards, output = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Copyright 2018, Stephen F Elston. All rights reserved. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
