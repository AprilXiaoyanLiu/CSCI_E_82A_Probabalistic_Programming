{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Bandit Problems\n",
    "\n",
    "## CSCI E-82A\n",
    "## Stephen Elston\n",
    "\n",
    "Interest in **bandit problems** has a long history, starting in the 1940s. Bandit problems seem simple, but actually can be complex and hard to understand. There are a great number of bandit algorithms. In this lesson we will only look at the simplest case of an **exploring bandit algorithm**.   \n",
    "\n",
    "Bandit algorithms are surprisingly useful in practice. For example, in the past 10 years a number of researchers have incorporated variations on bandit algorithms into recommender systems. Bandit algorithms are used along with or as an alternative to matrix factorization methods. See for example the paper by [Louedec, et. al.](file:///C:/Users/StevePC2/Downloads/10385-46145-1-PB.pdf).\n",
    "\n",
    "**Suggested reading:** An overview of several interesting and useful variations on bandit algorithms can be found in Chapter 2 or Sutton and Barto, second edition.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are Bandits?\n",
    "\n",
    "The name *Bandit* comes from the common name used for gambling slot machines. An example of a **one-armed** bandit in a casino is shown below.\n",
    "\n",
    "<img src=\"img/OneArmedBandit.JPG\" alt=\"Drawing\" style=\"width:300px; height:300px\"/>\n",
    "<center> **A Physical One Armed Badit** </center>  \n",
    "\n",
    " A player places a bet (inserts a token) into the bandit, and then pulls the lever. The player then receives a **reward**. The reward may be negative, the player has lost the amount bet. With some non-zero probability, a *lucky* player receives a positive reward and is considered to have won the game. It should come as no surprise that casinos set the probability of payout in a way that ensure *the house always wins* with high certainty. \n",
    " \n",
    " This idea can be extended to a conceptual **multi-armed** bandit. The multi-armed bandit works in much the same way as the one armed bandit. The agent tries to optimize the reward by **learning** a **policy** of which lever is most likely to pay out. The cartoon below illustrates this idea.  \n",
    "\n",
    "<img src=\"img/multiarmedbandit.JPG\" alt=\"Drawing\" style=\"width:300px; height:200px\"/>\n",
    "<center> **A Conceptual Muti-Armed Bandit and an Agent Learning Optimal Policy**    \n",
    "Attribution: Microsoft Research</center>  \n",
    "\n",
    "The bandit problem is a simple version of a **reinforcement learning** problem:\n",
    "- The agent must learn the model of the environment by trial and error. \n",
    "- The agent can take actions (pull a lever) in the environment. \n",
    "- The agent receives rewards (positive or negative) from the environment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Armed Bandit Model\n",
    "\n",
    "For this lesson we will use a simple bandit model. The agent received a positive reward of 1 with probability $p_k$ when pulling the kth lever. Otherwise the agent receives a reward of 0. We model this behavior as a series of **Bernoulli trials**. Bernoulli distributions model the outcomes of trials or experiments with binary outcomes. For example, pulling a lever two possible end states, $\\{ win:1,\\ loose:0 \\}$. \n",
    "\n",
    "For an event with a binary outcome, ${0,1}$ with probability $p$ of state 1, we can write the probability mass function for the Bernoulli distribution as:\n",
    "\n",
    "$$\n",
    "P(x\\ |\\ p) = \\bigg\\{ \n",
    "\\begin{matrix}\n",
    "p\\ if\\ x = 1\\\\\n",
    "(p - 1)\\ if\\ x = 0\n",
    "\\end{matrix}\\\\\n",
    "or\\\\\n",
    "P(x\\ |\\ p) = p^x(1 - p)^{(1-x)}\\ x \\in {0, 1}\n",
    "$$\n",
    "\n",
    "Our goal is to find an **optimal policy** which maximizes the **action value**. We say that the optimal policy, $q_*(a)$, gives the highest expected value for the action $a$:\n",
    "\n",
    "$$q_*(a) = \\mathbb{E}_{\\pi_*} [R_{t}\\ |\\ A_t = a] $$\n",
    "\n",
    "For our multi-armed bandit, $p_k$ can be different for each lever. The agent must therefore try to find the optimal policy of which lever(s) to pull to maximize reward. There are two approaches an agent can take:  \n",
    "1. Pull each lever a few times and then estimate $p_k$ for each lever. The agent then adopts the policy of pulling the best lever(s) exclusively. We say that this algorithm **exploits** the best known policy. The problem is, that $p_k$ is only an estimate and there is a significant chance that the best levers are not selected from a finite number of pulls.   \n",
    "2. Alternatively, the agent can use an **exploring algorithm**. The exploring algorithm does the following:\n",
    "  -  With some small probability, $\\epsilon$, the agent takes an **exploring action** by pulling a lever at random. The estimate of $p_k$ for the lever pulled is then updated. It is possible a better policy might be discovered during this exploring step. \n",
    "  - With probability $1 - \\epsilon$ the agent exploits the best known policy. \n",
    "  \n",
    "The second algorithm is known as an **$\\epsilon$-greedy** method. This algorithm exhibits a key trade-off in reinforcement learning, between exploration to improve policy and exploitation of the best known policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Computational Example\n",
    "\n",
    "Let's try an example of a 10-armed bandit problem. The code in the cell below imports the packages we will need and sets the probabilities $p_k$ for each of the 10 levers. Execute this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as nr\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "## initialize the probabilities for each arm\n",
    "probabilities = [0.24, 0.28, 0.32, 0.36, 0.40, 0.44, 0.48, 0.52, 0.56, 0.60]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below a function to simulate the Bernoulli trials for some number of lever pulls is defined. The probabilities for each lever simulated are defined in the list provided to the `probabilities` argument. \n",
    "\n",
    "Execute this code to simulate 2 pulls on each of the 10 levers and examine the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## define a \n",
    "def pull_arms(probs, pulls = 1, size = 1):\n",
    "    cols = len(list(probs))\n",
    "    if(pulls == 1):\n",
    "        out = nr.binomial(n = 1, p = probabilities, size = 10).reshape(1,cols)\n",
    "    else: \n",
    "        out = nr.binomial(n = 1, p = probabilities, size = 10).reshape(1,cols)\n",
    "        for _ in range(pulls - 1):\n",
    "            out = np.concatenate((out, nr.binomial(n = 1, p = probabilities, size = 10).reshape(1,cols)), axis = 0)\n",
    "    return out\n",
    "\n",
    "nr.seed(345)\n",
    "pull_arms(probabilities, pulls = 2)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As one should expect, the outcomes of the two pulls are quite different. \n",
    "\n",
    "Let's examine the distribution of the $p_k$ values for a small number of pulls on each lever. The function in the cell below calls the previously defined function. The realization from each sequence of 5 pulls is then used to compute $p_k$. A table of $p_k$ values is created. Execute the code which repeats this experiment 100 times and examine the result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Simulate estimated probabilities for a small number of pulls\n",
    "def compute_p_value(n_samples, probs, pulls_sample = 5):\n",
    "    cols = len(probs)\n",
    "    samples = pull_arms(probs, pulls_sample)\n",
    "    p = np.mean(samples, axis = 0).reshape(1,cols)\n",
    "    for _ in range(n_samples - 1):\n",
    "        samples = pull_arms(probs, pulls_sample)\n",
    "        temp = np.mean(samples, axis = 0).reshape(1,cols)\n",
    "        p = np.concatenate((p,temp), axis = 1)\n",
    "    return pd.DataFrame(p.reshape(n_samples, cols)) \n",
    "\n",
    "nr.seed(445)\n",
    "estimates = compute_p_value(100, probabilities)    \n",
    "estimates.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the head of this table, noticing that the values of $p_k$ are quite different from experiment to experiment. To get a feel for this variability, execute the code in the cell below to display violin plots of the distribution of $p_k$ for each lever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"whitegrid\")\n",
    "sns.violinplot(data = estimates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this chart you can see the considerable variation in the estimated values of $p_k$.\n",
    "\n",
    "## Exploitation Algorithm\n",
    "\n",
    "As a next step, let's try a simple exploitation algorithm. In the cell below, a function to find a policy is defined. The values of $p_k$ are estimated based on 5 pulls of each lever. Policy is determined by choosing the lever with the maximum $p_k$. **Ties are broken** by arbitrarily selecting the first lever in a list of possibilities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_max_lever(probabilities, pulls = 5):\n",
    "    vals = pull_arms(probabilities, pulls)\n",
    "    p = np.mean(vals, axis = 0)\n",
    "    max_p = np.max(p)\n",
    "    return np.where(p == max_p)[0][0]\n",
    "\n",
    "nr.seed(654)\n",
    "max_bandit = find_max_lever(probabilities)\n",
    "max_bandit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have found the *optimal policy* from the limited sample. The code in the cell below defines a function which simulates 1,000 pulls of the lever determined to be optimal. The average accumulated reward for each time step is computed by dividing the cumulative sum of rewards by the number of time steps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exploit(n_samps, p): \n",
    "    samples = []\n",
    "    for _ in range(n_samps):\n",
    "        samples.append(nr.binomial(n = 1, p = p, size = 1))\n",
    "    samples =  np.cumsum(np.concatenate(samples).ravel()) \n",
    "    n_samples = range(1, n_samps + 1)\n",
    "    proportion = [s/float(i+1) for i,s in zip(range(n_samps), samples)]\n",
    "    proportion[0] = 0.0\n",
    "    \n",
    "    return proportion\n",
    "\n",
    "prob_success = exploit(1000, probabilities[max_bandit]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the code in the cell below to plot the average reward accumulated at each time step.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(prob_success)\n",
    "plt.xlabel('Number of steps')\n",
    "plt.ylabel('Aerage reward')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you might expect, the average reward for this exploiting approach converges to the value of $p_k$ for the lever chosen for the policy. \n",
    "\n",
    "## Exploring Algorithm\n",
    "\n",
    "The function defined in the cell below implements a simple exploring algorithm. Policy is improve as the algorithm proceeds through the time steps. The algorithm works as follows:    \n",
    "1. The algorithm loops over a number of **episodes**. The mean of the accumulated rewards is computed over the results of each episode. \n",
    "2. Within each episode an initial random policy is selected.\n",
    "3. An inner loop iterates over the number of time steps of samples in each episode\n",
    "4. With probability $\\epsilon$ a random exploration step is taken. If the policy can be improved the new policy is adopted. \n",
    "5. The average accumulated rewards are computed. To limit memory use, the following commonly used update  expression is applied:   \n",
    "$$NewEstimate \\leftarrow OldEstimate + StepSize * \\big[Target - OldEstimate \\big]\\\\\n",
    "where\\ the\\ error\\ term\\ is\\\\\n",
    "\\big[Target - OldEstimate \\big]$$\n",
    "\n",
    "Specific details of this code can be learned by reading the comments. \n",
    "\n",
    "Execute the code for 100 episodes of 10,000 samples using $\\epsilon = 0.01$. The execution may take some time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def e_greedy(n_samps, episodes, epsilon, probabilities):\n",
    "    ## Array to keep the results for each step in each episode\n",
    "    proportion = np.zeros((episodes, n_samps))\n",
    "    n_probs = len(probabilities)\n",
    "    \n",
    "    for i in range(episodes):\n",
    "        ## Lists to maintain values of q, n by index k = lever identifier\n",
    "        q_n_k = np.zeros((2, n_probs))\n",
    "        \n",
    "        ## First randomly select a lever to pull\n",
    "        k_best = nr.randint(0,(n_probs-1))\n",
    "        p_best = probabilities[k_best]\n",
    "        \n",
    "        ## List for results by step\n",
    "        q_values = []\n",
    "        \n",
    "        for j in range(n_samps):\n",
    "            if(nr.uniform() <= epsilon): #Time to explore!\n",
    "                ## First randomly select a lever to pull\n",
    "                k_test = nr.randint(0,n_probs)\n",
    "                p_test = probabilities[k_test]\n",
    "                ## Pull the lever\n",
    "                r_n = nr.binomial(n = 1, p = p_test, size = 1)\n",
    "                ## Update the count\n",
    "                q_n_k[1,k_test] = q_n_k[1,k_test] + 1\n",
    "                ## Update Q\n",
    "                q_n_k[0,k_test]  = q_n_k[0,k_test] + (r_n - q_n_k[0,k_test])/q_n_k[1,k_test]\n",
    "                \n",
    "                ## Is the new Q better than the old.\n",
    "                ## If so, start using this level\n",
    "                if(q_n_k[0,k_test] > q_n_k[0,k_best]):\n",
    "                    k_best = k_test\n",
    "                    p_best = p_test\n",
    "                    \n",
    "            ## Exploit wth best lever\n",
    "            ## Pull the lever\n",
    "            r_n = nr.binomial(n = 1, p = p_best, size = 1)\n",
    "            ## Update the count\n",
    "            q_n_k[1,k_best] = q_n_k[1,k_best] + 1\n",
    "            ## Update Q\n",
    "            q_n_k[0,k_best]  = q_n_k[0,k_best] + (r_n - q_n_k[0,k_best])/q_n_k[1,k_best]\n",
    "            ## Append the Q value\n",
    "            q_values.append(q_n_k[0,k_best])    \n",
    "                \n",
    "        ## At the end of the episode save the Q values\n",
    "        proportion[i,:] = np.array(q_values).reshape((1, n_samps))\n",
    "                \n",
    "    return np.mean(proportion, axis = 0)\n",
    "\n",
    "nr.seed(7878)\n",
    "prob_success = e_greedy(10000, episodes = 100, epsilon = 0.01, probabilities = probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the code in the cell below to display a plot of the average reward by time step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(prob_success)\n",
    "plt.xlabel('Number of steps')\n",
    "plt.ylabel('Aerage reward')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The average reward slowly converges to the value of $p_k$ for the best lever. The policy is clearly being improved. \n",
    "\n",
    "Next, let's try another value of the exploitation-exploration trade-off parameter, $\\epsilon = 0.1$. In this case, every tenth step will be exploratory, whereas for the first example every 100th step was exploratory. The question is, how does this change performance of the algorithm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nr.seed(7879)\n",
    "prob_success = e_greedy(10000, episodes = 100, epsilon = 0.1, probabilities = probabilities)\n",
    "\n",
    "plt.plot(prob_success)\n",
    "plt.xlabel('Number of steps')\n",
    "plt.ylabel('Aerage reward')                        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With  $\\epsilon = 0.1$ convergence is much faster. However, on average one out of ten steps requires the extra work of exploration. In this case, the trade-off seems well worth it, since convergence appears to occur in about half as many time steps. \n",
    "\n",
    "Finally, it is worth checking how this algorithm behaves with  $\\epsilon = 0.0$, the pure exploitation case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nr.seed(7880)\n",
    "prob_success = e_greedy(10000, episodes = 100, epsilon = 0.0, probabilities = probabilities)\n",
    "\n",
    "plt.plot(prob_success)\n",
    "plt.xlabel('Number of steps')\n",
    "plt.ylabel('Aerage reward')                        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the result is a sub-optimal policy. Further, this policy cannot improve, since there is no exploration of alternatives. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Copyright 2018, Stephen F. Elston All rights reserved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
