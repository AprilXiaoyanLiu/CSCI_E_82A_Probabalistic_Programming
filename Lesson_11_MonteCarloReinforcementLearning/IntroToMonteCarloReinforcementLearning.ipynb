{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction To Monte Carlo Reinforcement Learning\n",
    "\n",
    "## CSCI E-82A\n",
    "\n",
    "## Stephen Elston\n",
    "\n",
    "Starting with this lesson, we will turn our attention to a **reinforcement learning**. Reinforcement learning is a distinctive type of machine learning and distinct from supervised learning and unsupervised learning. \n",
    "\n",
    "Reinforcement learning has several distinctive characteristics, which differentiate this method from other machine learning and dynamic programming:\n",
    "- **No Markov model** is required for reinforcement learning, in contrast to dynamic programming.\n",
    "- Like dynamic programming, reinforcement learning **optimizes a reward function**. This is in contrast to supervised and unsupervised learning which use an error or objective function.  \n",
    "- Reinforcement learning algorithms learn by **experience**. Over time, the algorithm learns a model of the environment and these results are used to optimize the expected reward. Learning from experience is in contrast to supervised learning which uses known marked cases. \n",
    "\n",
    "The interaction between a reinforcement learning agent and the environment are illustrated in the figure below. Notice that the only feedback the agent receives from the environment is the reward. There is no other evidence.   \n",
    "\n",
    "<img src=\"img/RL_Agent_Model.JPG\" alt=\"Drawing\" style=\"width:500px; height:300px\"/>\n",
    "<center> **Reinforcement Learning Agent and Environment** </center>  \n",
    "\n",
    "\n",
    "**Suggested readings** for Monte Carlo reinforcement learning Chapter 5 of Sutton and Barto provides a good introduction, including many alternative algorithms not discussed here.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of Monte Carlo Reinforcement Learning\n",
    "\n",
    "A wide variety of reinforcement learning algorithms have been developed over the past few decades. In this lesson we will explore the basics of the Monte Carlo method. Monte Carlo algorithms have been known for most of the history of reinforcement learning. However, they are generally considered inefficient for several reasons that will become apparent as we proceed:\n",
    "1. Monte Carlo methods rely large numbers of **random samples** to produce estimates. Thus, Monte Carlo algorithms are inherently computationally intensive. \n",
    "2. Monte Carlo reinforcement learning algorithms must **complete an entire episode** before an estimate can be produce. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basics of Monte Carlo Simulation\n",
    "\n",
    "Monte Carlo sampling was developed in the 1940s. Originally, Monte Carlo methods were used to compute estimates of complex functions which where analytically intractable. The basic idea is to **compute an estimate** of a complex function by **averaging a large number of samples**. \n",
    "\n",
    "Monte Carlo methods rely on the [**weak law of large numbers**](https://en.wikipedia.org/wiki/Law_of_large_numbers). The law of large numbers is a theorem that states that statistics of independent samples converge to the population values as more unbiased experiments are performed. We can write this mathematically for the **expected value** pr mean as:\n",
    "\n",
    "$$Let\\ \\bar{X} = \\frac{1}{n}\\sum_{i=1}^{n} X_i\\\\\n",
    "then\\ by\\ the\\ law\\ of\\ Large\\ Numbers\\\\\n",
    "\\bar{X} \\rightarrow E(X) = \\mu\\\\\n",
    "as\\\\\n",
    "n \\rightarrow \\infty$$\n",
    "\n",
    "Thus, if we sample some process $X$ enough times (possibly infinite), we can compute the expected value from these samples. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monte Carlo Reinforcement Learning\n",
    "\n",
    "But, how do we apply Monte Carlo sampling to reinforcement learning? More specifically, how do we apply Monte Carlo sampling to **episodic** reinforcement learning tasks. \n",
    "\n",
    "To understand this algorithm, it helps to examine the backup diagram shown below. This diagram shows Monte Carlo sampling of a single episode.    \n",
    "\n",
    "<img src=\"img/MC_Backup.JPG\" alt=\"Drawing\" style=\"width:75px; height:400px\"/>\n",
    "<center> **Backup Diagram for Monte Carlo Reinforcement Learning** </center>  \n",
    "\n",
    "Starting at the top of the diagram the system is in a state, s. An action, a, causes a transition to a new state. The sampling of the episode proceeds until the terminal state, t, is reached. The return for the initial state can only be computed once the Monte Carlo backup **ends at the terminal state**. In other words, **Monte Carlo algorithms do not bootstrap**.  \n",
    "\n",
    "In reinforcement learning we do not know the model. But, the agent can take a series of actions and find the rewards for these actions. For each episode the agent will accumulate the history of rewards for each action. \n",
    "\n",
    "Recall that for a finite or episodic Markov reward processes we define the **return** for state transitions starting with the current state. The return is the sum of the rewards for the $T$ future states transitions of the episodic process, and can be expressed as:\n",
    "\n",
    "$$G_t = R_{t+1} + R_{t+2} + \\ldots = R_{T}= \\sum_{k = 0}^{T} R_{t+k+1}$$ \n",
    "\n",
    "Thus, for any episode the Monte Carlo algorithm will sample the return for the states visited. Over a large (actually infinite) number of episodes the Monte Carlo algorithm will sample each action value several times. The sampled return values are then averaged for each state action. This process will converge to the actual action values, which are **unobservable** directly. \n",
    "\n",
    "For sampling a single episode of a Markov process, the Monte Carlo algorithm may or may not visit a state one or more times. The question then becomes, How should returns be computed if a state is visited more than once in an episode? There are two options each of which has different statical convergence properties:\n",
    "1. **First visit** Monte Carlo only estimates returns from the first visit to a state to termination. We will use first visit Monte Carlo in this lesson.\n",
    "2. **Every visit** Monte Carlo accumulates the returns for any visit to a state in an episode.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of First Visit Monte Carlo RL\n",
    "\n",
    "With this short introduction MC RL learning in mind, tet's try an example. We will sample the action value function using a simple MC algorithm here. \n",
    "\n",
    "**Navigation** to a goal is a significant problem in robotics. Real-world navigation is rather complex. Therefore, in this example we will use a simple analog called a **grid world**. The grid world for this problem is shown below. \n",
    "\n",
    "<img src=\"img/GridWorld.JPG\" alt=\"Drawing\" style=\"width:200px; height:200px\"/>\n",
    "<center> **A 4x4 Grid World with Terminal State** </center>\n",
    "\n",
    "The grid world consists of a 4x4 set of positions the robot can occupy. Each position is considered a state. The goal is to navigate to state 0, the goal, in the minimum steps. We will explore methods to find policies which reach this goal and achieve maximum reward. \n",
    "\n",
    "Grid position 0 is the goal and a **terminal state**. There are no possible state transitions out of this position. The presence of a terminal state makes this an **episodic Markov random process**. For each episode sampled the robot can start in any other random position, $\\{ 1,2,3,4,5,6,7,8,9,10,11,12,13,14,15 \\}$. This random selection process makes this a **random start** Monte Carlo algorithm. The episode terminates when the robot enters the terminal position (state 0).  \n",
    "\n",
    "### Representations For MC RL\n",
    "\n",
    "In each state, there are four possible actions the robot can take:\n",
    "- up, u\n",
    "- down, d,\n",
    "- left, l\n",
    "- right, r\n",
    "\n",
    "The MC RL agent has no model. Therefore, beyond these allowed actions, all other information is encapsulated in the environment and is unobservable by the agent. This is the key difference between reinforcement learning and dynamic programming. \n",
    "\n",
    "In reality, an RL agent may need to explore to find the possible actions when it is in some particular state. To simplify our example, we encode, or represent, these possibilities in a dictionary as shown in the code block below. We use a dictionary of dictionaries to perform the lookup. The keys of the outer dictionary are the identifiers (numbers) of the states. The keys of the inner dictionary are the possible actions and the values are the **successor state**, $s'$, for that transition.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import numpy for latter\n",
    "import numpy as np\n",
    "import numpy.random as nr\n",
    "\n",
    "## Define the transition dictonary of dictionaries:\n",
    "neighbors = {0:{'u':0, 'd':0, 'l':0, 'r':0},\n",
    "          1:{'u':1, 'd':5, 'l':0, 'r':2},\n",
    "          2:{'u':2, 'd':6, 'l':1, 'r':3},\n",
    "          3:{'u':3, 'd':7, 'l':2, 'r':3},\n",
    "          4:{'u':0, 'd':8, 'l':4, 'r':5},\n",
    "          5:{'u':1, 'd':9, 'l':4, 'r':6},\n",
    "          6:{'u':2, 'd':10, 'l':5, 'r':7},\n",
    "          7:{'u':3, 'd':11, 'l':6, 'r':7},\n",
    "          8:{'u':4, 'd':12, 'l':8, 'r':9},\n",
    "          9:{'u':5, 'd':13, 'l':8, 'r':10},\n",
    "          10:{'u':6, 'd':14, 'l':9, 'r':11},\n",
    "          11:{'u':7, 'd':15, 'l':10, 'r':11},\n",
    "          12:{'u':8, 'd':12, 'l':12, 'r':13},\n",
    "          13:{'u':9, 'd':13, 'l':12, 'r':14},\n",
    "          14:{'u':10, 'd':14, 'l':13, 'r':15},\n",
    "          15:{'u':11, 'd':15, 'l':14, 'r':15}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When performing MC RL, we can start with an arbitrary initial policy. The MC RL agent will then improve this policy and in the process will **learn the Markov process model**. Again, this is a key difference with dynamic programming where this model is specified. \n",
    "\n",
    "We need to define the transition probabilities for the initial policy. We set the probabilities for each transition as a **uniform distribution** leading to random action by the robot. As there are 4 possible transitions from each state, this means all transition probabilities are 0.25. In other words, this is a random policy which does not favor any particular plan. \n",
    "\n",
    "The initial uniform transition probabilities are encoded using a dictionary of dictionaries. The organization of this data structure is identical to the foregoing data structure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = {0:{'u':0.0, 'd':0.0, 'l':0.0, 'r':0.0},\n",
    "                        1:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25}, \n",
    "                        2:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                        3:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                        4:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                        5:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                        6:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                        7:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                        8:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                        9:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                        10:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                        11:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                        12:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                        13:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                        14:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                        15:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25}}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The robot receives the following rewards:\n",
    "- 10 for entering position 0. \n",
    "- -1 for attempting to leave the grid. In other words, we penalize the robot for hitting the edges of the grid.  \n",
    "- -0.1 for all other state transitions, which is the cost for the robot to move from one state to another. If we did not have this penalty, the robot could follow any random plan to the goal which did not hit the edges. \n",
    "\n",
    "This **reward structure is unknown to the MC RL agent**. The agent must **learn** the rewards by sampling the environment. Here the rewards are in the form of action values. We encode these rewards in the same type of dictionary structure used for the foregoing structures. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = {0:{'u':0.0, 'd':0.0, 'l':0.0, 'r':0.0},\n",
    "          1:{'u':-1, 'd':-0.1, 'l':10.0, 'r':-0.1},\n",
    "          2:{'u':-1.0, 'd':-0.1, 'l':-0.1, 'r':-0.1},\n",
    "          3:{'u':-1.0, 'd':-0.1, 'l':-0.1, 'r':-1.0},\n",
    "          4:{'u':10.0, 'd':-0.1, 'l':-1.0, 'r':-0.1},\n",
    "          5:{'u':-0.1, 'd':-0.1, 'l':-0.1, 'r':-0.1},\n",
    "          6:{'u':-0.1, 'd':-0.1, 'l':-0.1, 'r':-0.1},\n",
    "          7:{'u':-0.1, 'd':-0.1, 'l':-0.1, 'r':-1.0},\n",
    "          8:{'u':-0.1, 'd':-0.1, 'l':-1.0, 'r':-0.1},\n",
    "          9:{'u':-0.1, 'd':-0.1, 'l':-0.1, 'r':-0.1},\n",
    "          10:{'u':-0.1, 'd':-0.1, 'l':-0.1, 'r':-0.1},\n",
    "          11:{'u':-0.1, 'd':-0.1, 'l':-0.1, 'r':-1.0},\n",
    "          12:{'u':-0.1, 'd':-1.0, 'l':-1.0, 'r':-0.1},\n",
    "          13:{'u':-0.1, 'd':-1.0, 'l':-0.1, 'r':-0.1},\n",
    "          14:{'u':-0.1, 'd':-1.0, 'l':-0.1, 'r':-0.1},\n",
    "          15:{'u':-0.1, 'd':-1.0, 'l':-0.1, 'r':-1.0}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MC RL agent will sample state action values in a random manner. The RL agent does not know how many states there are and which states are terminal. Thus, the agent performs a random walk until the terminal state is reached. The probabilities of a particular state transition are determined by the current policy. \n",
    "\n",
    "### MC Policy Evaluation\n",
    "\n",
    "We are using random start MC. The code in the cell below generates a random walk from the starting state to the terminal state. This process is **not part of the agent** since it does not know which state transitions are possible from its current state. You can understand the details of this function by reading the code comments. Execute this code and examine the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MC_generate_episode(policy, neighbors, terminal):\n",
    "    ## List of states which might be visited in episode\n",
    "    n_states = len(policy)\n",
    "#    visited_state = [0] * n_states\n",
    "    states = list(neighbors.keys())\n",
    "    \n",
    "    ## Random starting state for this episode, but can't be the terminal state\n",
    "    current_state = nr.choice(states, size = 1)[0]\n",
    "    while(current_state == terminal): # Keep trying to not use terminal state to start\n",
    "        current_state = nr.choice(states, size = 1)[0]\n",
    "            \n",
    "    ## Take a random walk trough the states until we get to the terminal state\n",
    "    ## We do some bookkeeping to ensure we only visit states once.\n",
    "    visited = [] # List of states visited on random walk\n",
    "    while(current_state != terminal): # Stop when at terminal state\n",
    "        ## Probability of state transition given policy\n",
    "        probs = list(policy[current_state].values())\n",
    "        ## Find next state to transition to\n",
    "        next_state = nr.choice(list(neighbors[current_state].values()), size = 1, p = probs)[0]\n",
    "        visited.append(next_state)\n",
    "        current_state = next_state  \n",
    "    return(visited)    \n",
    "    \n",
    "nr.seed(4567)    \n",
    "MC_generate_episode(policy, neighbors, 0) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The random walk generated may visit the same state a number of times. Eventually the terminal state, 0, is reached and the walk is over. \n",
    "\n",
    "The random walk generated by the above function defines one episode. By generating a large number of episodes we can perform **Monte Carlo policy evaluation**. For each episode the returns for each state visited are accumulated, starting with the **first visit**. Once all episodes have concluded, the average returns are computed. \n",
    "\n",
    "Execute this code to evaluate the initial uniformly distributed policy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def MC_state_values(policy, neighbors, rewards, terminal, episodes = 1):\n",
    "    '''Function for first visit Monte Carlo on GridWorld.'''\n",
    "    ## Create list of states \n",
    "    states = list(policy.keys())\n",
    "    n_states = len(states)\n",
    "    \n",
    "    ## An array to hold the accumulated returns as we visit states\n",
    "    G = np.zeros((episodes,n_states))\n",
    "    \n",
    "    ## An array to keep track of how many times we visit each state so we can \n",
    "    ## compute the mean\n",
    "    n_visits = np.zeros((n_states))\n",
    "    \n",
    "    ## Iterate over the episodes\n",
    "    for i in range(episodes):\n",
    "        ## For each episode we use a list to keep track of states we have visited.\n",
    "        ## Once we visit a state we need to accumulate values to get the returns\n",
    "        states_visited = []\n",
    "   \n",
    "        ## Get a path for this episode\n",
    "        visit_list = MC_generate_episode(policy, neighbors, terminal)\n",
    "        current_state = visit_list[0]\n",
    "        for state in visit_list[0:]: \n",
    "            ## list of states we can transition to from current state\n",
    "            transition_list = list(neighbors[current_state].values())\n",
    "            \n",
    "            if(state in transition_list): # Make sure the transistion is allowed\n",
    "                transition_index = transition_list.index(state)   \n",
    "  \n",
    "                ## find the action value for the state transition\n",
    "                v_s = list(rewards[current_state].values())[transition_index]\n",
    "   \n",
    "                ## Mark that the current state has been visited \n",
    "                if(state not in states_visited): states_visited.append(current_state)  \n",
    "                ## Loop over the states already visited to add the value to the return\n",
    "                for visited in states_visited:\n",
    "                    G[i,visited] = G[i,visited] + v_s\n",
    "                    n_visits[visited] = n_visits[visited] + 1.0\n",
    "            ## Update the current state for next transition\n",
    "            current_state = state   \n",
    "    \n",
    "    ## Compute the average of G over the episodes are return\n",
    "    n_visits = [nv if nv != 0.0 else 1.0 for nv in n_visits]\n",
    "    returns = np.divide(np.sum(G, axis = 0), n_visits)   \n",
    "    return(returns)              \n",
    "    \n",
    "#nr.seed(335)\n",
    "returns = MC_state_values(policy, neighbors, rewards, terminal = 0, episodes = 1000)\n",
    "np.array(returns).reshape((4,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results look promising. The returns become smaller the further the state is from the goal. \n",
    "\n",
    "\n",
    "### Policy Improvement\n",
    "\n",
    "RL MC uses the idea of **generalized policy improvement**. Recall that GPI divides the policy improvement and evaluation steps into opposing processes and iterates between them. This process can be done in a quite granular way, even on a single episode or even state at a time. The figure below illustrates the concept of GPI.  \n",
    "\n",
    "<img src=\"img/GPI.JPG\" alt=\"Drawing\" style=\"width:250px; height:250px\"/>\n",
    "<center> **Concept of Generalized Policy Improvement** </center>\n",
    "\n",
    "The code in the cell below uses the GPI method to improve the policy for the grid world using GPI. The outer loop or cycle performs one iteration of GPI which has two steps: \n",
    "1. At the start of the loop the returns for the current policy are evaluated. In this case we use the average of several MC episodes. \n",
    "2. Next, the policy is updated using the new return values. The policy is improved by increasing the transition probabilities for actions (transitions) with higher reward. To ensure that the algorithm **continues to explore**, transition probabilities are never set to 0 but rather to a small minimum value $\\epsilon$. \n",
    " \n",
    "Addition details on the operation of this algorithm can be obtained by reading the comments. Execute this code and examine the results.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "def MC_optimal_policy(policy, neighbors, rewards, terminal, episodes = 10, cycles = 10, epsilon = 0.05):\n",
    "    ## Create a working cooy of the initial policy\n",
    "    current_policy = copy.deepcopy(policy)\n",
    "    \n",
    "    ## Loop over a number of cycles of GPI\n",
    "    for _ in range(cycles):\n",
    "        ## First compute the average returns for each of the states. \n",
    "        ## This is the policy evaluation phase\n",
    "        returns = MC_state_values(current_policy, neighbors, rewards, terminal = terminal, episodes = episodes)\n",
    "        \n",
    "        ## We want max Q for each state, where Q is just the difference \n",
    "        ## in the values of the possible state transition\n",
    "        ## This is the policy evaluation phase\n",
    "        for s in current_policy.keys(): # iterate over all states\n",
    "            ## Compute Q for each possible state transistion\n",
    "            ## Start by creating a list of the adjacent states.\n",
    "            possible_s_prime = neighbors[s]\n",
    "            neighbor_states = list(possible_s_prime.values())\n",
    "            ## Check if terminal state is neighbor, but state is not terminal.\n",
    "            if(terminal in neighbor_states and s != terminal):\n",
    "                ## account for the special case adjacent to goal\n",
    "                neighbor_Q = []\n",
    "                for s_prime in possible_s_prime.keys(): # Iterate over adjacent states\n",
    "                    if(neighbors[s][s_prime] == terminal):  \n",
    "                         neighbor_Q.append(returns[s])\n",
    "                    else: neighbor_Q.append(0.0) ## Other transisions have 0 value.   \n",
    "            else: \n",
    "                 ## The other case is rather easy. Compute Q for the transistion to each neighbor           \n",
    "                 neighbor_values = returns[neighbor_states]\n",
    "                 neighbor_Q = [n_val - returns[s] for n_val in neighbor_values]\n",
    "                \n",
    "            ## Find the index for the state transistions with the largest values \n",
    "            ## May be more than one. \n",
    "            max_index = np.where(np.array(neighbor_Q) == max(neighbor_Q))[0]  \n",
    "            \n",
    "            ## Probabilities of transition\n",
    "            ## Need to allow for further exploration so don't let any \n",
    "            ## transition probability be 0.\n",
    "            ## Some gymnastics are required to ensure that the probabilities \n",
    "            ## over the transistions actual add to exactly 1.0\n",
    "            neighbors_len = float(len(np.array(neighbor_Q)))\n",
    "            max_len = float(len(max_index))\n",
    "            diff = round(neighbors_len - max_len,3)\n",
    "            prob_for_policy = round(1.0/max_len,3)\n",
    "            adjust = round((epsilon * (diff)), 3)\n",
    "            prob_for_policy = prob_for_policy - adjust\n",
    "            if(diff != 0.0):\n",
    "                remainder = (1.0 - max_len * prob_for_policy)/diff\n",
    "            else:\n",
    "                remainder = epsilon\n",
    "                                                 \n",
    "            for i, key in enumerate(current_policy[s]): ## Update policy\n",
    "                if(i in max_index): current_policy[s][key] = prob_for_policy\n",
    "                else: current_policy[s][key] = remainder          \n",
    "                   \n",
    "    return current_policy\n",
    " \n",
    "nr.seed(9876)    \n",
    "MC_policy = MC_optimal_policy(policy, neighbors, rewards, terminal = 0, episodes = 100, cycles = 20, \n",
    "                              epsilon = 0.01)  \n",
    "MC_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The improved policy makes sense. Transitions that move the robot closer to the goal are favored. \n",
    "\n",
    "Finally, execute the code in the cell below to compute the returns for the improved policy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nr.seed(369)\n",
    "returns = MC_state_values(MC_policy, neighbors, rewards, terminal = 0, episodes = 1000)\n",
    "np.array(returns).reshape((4,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These returns are significantly higher than for the random policy, indicating the policy is indeed an improvement. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Copyright 2018, Stephen F Elston. All rights reserved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
